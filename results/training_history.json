[
  {
    "loss": 3.7292,
    "grad_norm": 0.8492225408554077,
    "learning_rate": 0.0,
    "epoch": 0.0044004400440044,
    "step": 1
  },
  {
    "loss": 3.8348,
    "grad_norm": 0.6212777495384216,
    "learning_rate": 4e-05,
    "epoch": 0.0088008800880088,
    "step": 2
  },
  {
    "loss": 3.8974,
    "grad_norm": 0.6654035449028015,
    "learning_rate": 8e-05,
    "epoch": 0.013201320132013201,
    "step": 3
  },
  {
    "loss": 3.7205,
    "grad_norm": 0.507657527923584,
    "learning_rate": 0.00012,
    "epoch": 0.0176017601760176,
    "step": 4
  },
  {
    "loss": 3.5348,
    "grad_norm": 0.6340736150741577,
    "learning_rate": 0.00016,
    "epoch": 0.022002200220022004,
    "step": 5
  },
  {
    "loss": 3.6848,
    "grad_norm": 0.4957958459854126,
    "learning_rate": 0.0002,
    "epoch": 0.026402640264026403,
    "step": 6
  },
  {
    "loss": 3.3854,
    "grad_norm": 0.5952958464622498,
    "learning_rate": 0.0001996638655462185,
    "epoch": 0.030803080308030802,
    "step": 7
  },
  {
    "loss": 3.3836,
    "grad_norm": 0.5243536829948425,
    "learning_rate": 0.00019932773109243698,
    "epoch": 0.0352035203520352,
    "step": 8
  },
  {
    "loss": 3.4026,
    "grad_norm": 0.7174702882766724,
    "learning_rate": 0.00019899159663865548,
    "epoch": 0.039603960396039604,
    "step": 9
  },
  {
    "loss": 3.3064,
    "grad_norm": 0.613244891166687,
    "learning_rate": 0.00019865546218487395,
    "epoch": 0.04400440044004401,
    "step": 10
  },
  {
    "loss": 3.3703,
    "grad_norm": 0.6646502017974854,
    "learning_rate": 0.00019831932773109245,
    "epoch": 0.0484048404840484,
    "step": 11
  },
  {
    "loss": 3.1555,
    "grad_norm": 0.6498649716377258,
    "learning_rate": 0.00019798319327731095,
    "epoch": 0.052805280528052806,
    "step": 12
  },
  {
    "loss": 3.3688,
    "grad_norm": 0.6434691548347473,
    "learning_rate": 0.00019764705882352942,
    "epoch": 0.05720572057205721,
    "step": 13
  },
  {
    "loss": 3.2317,
    "grad_norm": 0.5132766366004944,
    "learning_rate": 0.00019731092436974792,
    "epoch": 0.061606160616061605,
    "step": 14
  },
  {
    "loss": 3.3771,
    "grad_norm": 0.5968378782272339,
    "learning_rate": 0.00019697478991596642,
    "epoch": 0.066006600660066,
    "step": 15
  },
  {
    "loss": 3.312,
    "grad_norm": 0.5924370884895325,
    "learning_rate": 0.00019663865546218486,
    "epoch": 0.0704070407040704,
    "step": 16
  },
  {
    "loss": 3.004,
    "grad_norm": 0.561390221118927,
    "learning_rate": 0.00019630252100840336,
    "epoch": 0.0748074807480748,
    "step": 17
  },
  {
    "loss": 3.1125,
    "grad_norm": 0.5268422365188599,
    "learning_rate": 0.00019596638655462186,
    "epoch": 0.07920792079207921,
    "step": 18
  },
  {
    "loss": 3.2247,
    "grad_norm": 0.5370637774467468,
    "learning_rate": 0.00019563025210084033,
    "epoch": 0.08360836083608361,
    "step": 19
  },
  {
    "loss": 3.2078,
    "grad_norm": 0.576687753200531,
    "learning_rate": 0.00019529411764705883,
    "epoch": 0.08800880088008801,
    "step": 20
  },
  {
    "loss": 3.2954,
    "grad_norm": 0.5489393472671509,
    "learning_rate": 0.0001949579831932773,
    "epoch": 0.0924092409240924,
    "step": 21
  },
  {
    "loss": 3.31,
    "grad_norm": 0.4649794101715088,
    "learning_rate": 0.0001946218487394958,
    "epoch": 0.0968096809680968,
    "step": 22
  },
  {
    "loss": 3.1683,
    "grad_norm": 0.506482720375061,
    "learning_rate": 0.0001942857142857143,
    "epoch": 0.10121012101210121,
    "step": 23
  },
  {
    "loss": 3.1576,
    "grad_norm": 0.5672658085823059,
    "learning_rate": 0.00019394957983193278,
    "epoch": 0.10561056105610561,
    "step": 24
  },
  {
    "loss": 3.2909,
    "grad_norm": 0.5406337976455688,
    "learning_rate": 0.00019361344537815127,
    "epoch": 0.11001100110011001,
    "step": 25
  },
  {
    "loss": 3.3713,
    "grad_norm": 0.5279222130775452,
    "learning_rate": 0.00019327731092436975,
    "epoch": 0.11441144114411442,
    "step": 26
  },
  {
    "loss": 3.4273,
    "grad_norm": 0.6035106182098389,
    "learning_rate": 0.00019294117647058825,
    "epoch": 0.1188118811881188,
    "step": 27
  },
  {
    "loss": 3.0947,
    "grad_norm": 0.47237884998321533,
    "learning_rate": 0.00019260504201680674,
    "epoch": 0.12321232123212321,
    "step": 28
  },
  {
    "loss": 2.8565,
    "grad_norm": 0.5090148448944092,
    "learning_rate": 0.00019226890756302522,
    "epoch": 0.1276127612761276,
    "step": 29
  },
  {
    "loss": 3.2723,
    "grad_norm": 0.4875279366970062,
    "learning_rate": 0.00019193277310924372,
    "epoch": 0.132013201320132,
    "step": 30
  },
  {
    "loss": 3.1896,
    "grad_norm": 0.49126240611076355,
    "learning_rate": 0.00019159663865546221,
    "epoch": 0.13641364136413642,
    "step": 31
  },
  {
    "loss": 2.9827,
    "grad_norm": 0.4825531840324402,
    "learning_rate": 0.0001912605042016807,
    "epoch": 0.1408140814081408,
    "step": 32
  },
  {
    "loss": 3.2245,
    "grad_norm": 0.5640914440155029,
    "learning_rate": 0.00019092436974789919,
    "epoch": 0.14521452145214522,
    "step": 33
  },
  {
    "loss": 3.1893,
    "grad_norm": 0.5238455533981323,
    "learning_rate": 0.00019058823529411766,
    "epoch": 0.1496149614961496,
    "step": 34
  },
  {
    "loss": 3.1352,
    "grad_norm": 0.5207211375236511,
    "learning_rate": 0.00019025210084033613,
    "epoch": 0.15401540154015403,
    "step": 35
  },
  {
    "loss": 3.2774,
    "grad_norm": 0.5547874569892883,
    "learning_rate": 0.00018991596638655463,
    "epoch": 0.15841584158415842,
    "step": 36
  },
  {
    "loss": 3.122,
    "grad_norm": 0.552807629108429,
    "learning_rate": 0.0001895798319327731,
    "epoch": 0.1628162816281628,
    "step": 37
  },
  {
    "loss": 3.0435,
    "grad_norm": 0.6379682421684265,
    "learning_rate": 0.0001892436974789916,
    "epoch": 0.16721672167216722,
    "step": 38
  },
  {
    "loss": 3.0762,
    "grad_norm": 0.6240447163581848,
    "learning_rate": 0.0001889075630252101,
    "epoch": 0.1716171617161716,
    "step": 39
  },
  {
    "loss": 2.9759,
    "grad_norm": 0.6112969517707825,
    "learning_rate": 0.00018857142857142857,
    "epoch": 0.17601760176017603,
    "step": 40
  },
  {
    "loss": 3.1356,
    "grad_norm": 0.5192984938621521,
    "learning_rate": 0.00018823529411764707,
    "epoch": 0.18041804180418042,
    "step": 41
  },
  {
    "loss": 3.0752,
    "grad_norm": 0.5707442164421082,
    "learning_rate": 0.00018789915966386554,
    "epoch": 0.1848184818481848,
    "step": 42
  },
  {
    "loss": 2.9224,
    "grad_norm": 0.5970823168754578,
    "learning_rate": 0.00018756302521008404,
    "epoch": 0.18921892189218922,
    "step": 43
  },
  {
    "loss": 3.262,
    "grad_norm": 0.5775326490402222,
    "learning_rate": 0.00018722689075630254,
    "epoch": 0.1936193619361936,
    "step": 44
  },
  {
    "loss": 3.115,
    "grad_norm": 0.6004844903945923,
    "learning_rate": 0.000186890756302521,
    "epoch": 0.19801980198019803,
    "step": 45
  },
  {
    "loss": 3.1841,
    "grad_norm": 0.520995557308197,
    "learning_rate": 0.0001865546218487395,
    "epoch": 0.20242024202420242,
    "step": 46
  },
  {
    "loss": 3.0371,
    "grad_norm": 0.7665164470672607,
    "learning_rate": 0.000186218487394958,
    "epoch": 0.2068206820682068,
    "step": 47
  },
  {
    "loss": 3.201,
    "grad_norm": 0.5623266696929932,
    "learning_rate": 0.00018588235294117648,
    "epoch": 0.21122112211221122,
    "step": 48
  },
  {
    "loss": 3.0105,
    "grad_norm": 0.5488765239715576,
    "learning_rate": 0.00018554621848739498,
    "epoch": 0.2156215621562156,
    "step": 49
  },
  {
    "loss": 3.2136,
    "grad_norm": 0.6951053738594055,
    "learning_rate": 0.00018521008403361345,
    "epoch": 0.22002200220022003,
    "step": 50
  },
  {
    "loss": 2.8117,
    "grad_norm": 0.6363223791122437,
    "learning_rate": 0.00018487394957983195,
    "epoch": 0.22442244224422442,
    "step": 51
  },
  {
    "loss": 3.0248,
    "grad_norm": 0.6477840542793274,
    "learning_rate": 0.00018453781512605045,
    "epoch": 0.22882288228822883,
    "step": 52
  },
  {
    "loss": 3.2079,
    "grad_norm": 0.755526065826416,
    "learning_rate": 0.0001842016806722689,
    "epoch": 0.23322332233223322,
    "step": 53
  },
  {
    "loss": 3.0517,
    "grad_norm": 0.685981273651123,
    "learning_rate": 0.0001838655462184874,
    "epoch": 0.2376237623762376,
    "step": 54
  },
  {
    "loss": 3.0411,
    "grad_norm": 0.6046358346939087,
    "learning_rate": 0.0001835294117647059,
    "epoch": 0.24202420242024203,
    "step": 55
  },
  {
    "loss": 3.1792,
    "grad_norm": 0.6410549879074097,
    "learning_rate": 0.00018319327731092437,
    "epoch": 0.24642464246424642,
    "step": 56
  },
  {
    "loss": 3.0316,
    "grad_norm": 0.6462271213531494,
    "learning_rate": 0.00018285714285714286,
    "epoch": 0.2508250825082508,
    "step": 57
  },
  {
    "loss": 3.1312,
    "grad_norm": 0.5307733416557312,
    "learning_rate": 0.00018252100840336134,
    "epoch": 0.2552255225522552,
    "step": 58
  },
  {
    "loss": 3.1151,
    "grad_norm": 0.602904736995697,
    "learning_rate": 0.00018218487394957984,
    "epoch": 0.25962596259625964,
    "step": 59
  },
  {
    "loss": 3.1712,
    "grad_norm": 0.6889688968658447,
    "learning_rate": 0.00018184873949579833,
    "epoch": 0.264026402640264,
    "step": 60
  },
  {
    "loss": 3.134,
    "grad_norm": 0.5574544072151184,
    "learning_rate": 0.0001815126050420168,
    "epoch": 0.2684268426842684,
    "step": 61
  },
  {
    "loss": 3.1212,
    "grad_norm": 0.5183242559432983,
    "learning_rate": 0.0001811764705882353,
    "epoch": 0.27282728272827284,
    "step": 62
  },
  {
    "loss": 3.0474,
    "grad_norm": 0.7097542881965637,
    "learning_rate": 0.0001808403361344538,
    "epoch": 0.27722772277227725,
    "step": 63
  },
  {
    "loss": 3.18,
    "grad_norm": 0.6061513423919678,
    "learning_rate": 0.00018050420168067228,
    "epoch": 0.2816281628162816,
    "step": 64
  },
  {
    "loss": 2.9766,
    "grad_norm": 0.5853066444396973,
    "learning_rate": 0.00018016806722689078,
    "epoch": 0.28602860286028603,
    "step": 65
  },
  {
    "loss": 3.1121,
    "grad_norm": 0.5585669875144958,
    "learning_rate": 0.00017983193277310925,
    "epoch": 0.29042904290429045,
    "step": 66
  },
  {
    "loss": 3.0546,
    "grad_norm": 0.5822581648826599,
    "learning_rate": 0.00017949579831932775,
    "epoch": 0.2948294829482948,
    "step": 67
  },
  {
    "loss": 2.9823,
    "grad_norm": 0.6888179183006287,
    "learning_rate": 0.00017915966386554625,
    "epoch": 0.2992299229922992,
    "step": 68
  },
  {
    "loss": 2.907,
    "grad_norm": 0.6262770891189575,
    "learning_rate": 0.00017882352941176472,
    "epoch": 0.30363036303630364,
    "step": 69
  },
  {
    "loss": 3.1394,
    "grad_norm": 0.5857532620429993,
    "learning_rate": 0.00017848739495798322,
    "epoch": 0.30803080308030806,
    "step": 70
  },
  {
    "loss": 3.2069,
    "grad_norm": 0.7619343996047974,
    "learning_rate": 0.0001781512605042017,
    "epoch": 0.3124312431243124,
    "step": 71
  },
  {
    "loss": 3.1443,
    "grad_norm": 0.6479030847549438,
    "learning_rate": 0.00017781512605042016,
    "epoch": 0.31683168316831684,
    "step": 72
  },
  {
    "loss": 3.2303,
    "grad_norm": 0.9147855639457703,
    "learning_rate": 0.00017747899159663866,
    "epoch": 0.32123212321232125,
    "step": 73
  },
  {
    "loss": 2.958,
    "grad_norm": 0.650357723236084,
    "learning_rate": 0.00017714285714285713,
    "epoch": 0.3256325632563256,
    "step": 74
  },
  {
    "loss": 2.9865,
    "grad_norm": 0.7161633968353271,
    "learning_rate": 0.00017680672268907563,
    "epoch": 0.33003300330033003,
    "step": 75
  },
  {
    "loss": 3.041,
    "grad_norm": 0.808866024017334,
    "learning_rate": 0.00017647058823529413,
    "epoch": 0.33443344334433445,
    "step": 76
  },
  {
    "loss": 2.9589,
    "grad_norm": 0.6971254348754883,
    "learning_rate": 0.0001761344537815126,
    "epoch": 0.3388338833883388,
    "step": 77
  },
  {
    "loss": 3.2223,
    "grad_norm": 0.74379962682724,
    "learning_rate": 0.0001757983193277311,
    "epoch": 0.3432343234323432,
    "step": 78
  },
  {
    "loss": 3.0513,
    "grad_norm": 0.8712570071220398,
    "learning_rate": 0.0001754621848739496,
    "epoch": 0.34763476347634764,
    "step": 79
  },
  {
    "loss": 3.2033,
    "grad_norm": 0.7006435394287109,
    "learning_rate": 0.00017512605042016807,
    "epoch": 0.35203520352035206,
    "step": 80
  },
  {
    "loss": 2.9464,
    "grad_norm": 0.7256669998168945,
    "learning_rate": 0.00017478991596638657,
    "epoch": 0.3564356435643564,
    "step": 81
  },
  {
    "loss": 3.17,
    "grad_norm": 0.7596362233161926,
    "learning_rate": 0.00017445378151260504,
    "epoch": 0.36083608360836084,
    "step": 82
  },
  {
    "loss": 3.4102,
    "grad_norm": 0.810372531414032,
    "learning_rate": 0.00017411764705882354,
    "epoch": 0.36523652365236525,
    "step": 83
  },
  {
    "loss": 3.1447,
    "grad_norm": 0.6866142749786377,
    "learning_rate": 0.00017378151260504204,
    "epoch": 0.3696369636963696,
    "step": 84
  },
  {
    "loss": 2.9381,
    "grad_norm": 0.7750453352928162,
    "learning_rate": 0.0001734453781512605,
    "epoch": 0.37403740374037403,
    "step": 85
  },
  {
    "loss": 2.8988,
    "grad_norm": 0.5962628126144409,
    "learning_rate": 0.000173109243697479,
    "epoch": 0.37843784378437845,
    "step": 86
  },
  {
    "loss": 3.0494,
    "grad_norm": 0.7012642025947571,
    "learning_rate": 0.00017277310924369748,
    "epoch": 0.38283828382838286,
    "step": 87
  },
  {
    "loss": 3.0745,
    "grad_norm": 0.7698298096656799,
    "learning_rate": 0.00017243697478991598,
    "epoch": 0.3872387238723872,
    "step": 88
  },
  {
    "loss": 3.1906,
    "grad_norm": 0.6644094586372375,
    "learning_rate": 0.00017210084033613448,
    "epoch": 0.39163916391639164,
    "step": 89
  },
  {
    "loss": 3.009,
    "grad_norm": 0.8487237691879272,
    "learning_rate": 0.00017176470588235293,
    "epoch": 0.39603960396039606,
    "step": 90
  },
  {
    "loss": 2.9243,
    "grad_norm": 0.6809531450271606,
    "learning_rate": 0.00017142857142857143,
    "epoch": 0.4004400440044004,
    "step": 91
  },
  {
    "loss": 3.1008,
    "grad_norm": 0.7746023535728455,
    "learning_rate": 0.00017109243697478992,
    "epoch": 0.40484048404840484,
    "step": 92
  },
  {
    "loss": 2.8955,
    "grad_norm": 0.837928056716919,
    "learning_rate": 0.0001707563025210084,
    "epoch": 0.40924092409240925,
    "step": 93
  },
  {
    "loss": 2.9854,
    "grad_norm": 0.6053668856620789,
    "learning_rate": 0.0001704201680672269,
    "epoch": 0.4136413641364136,
    "step": 94
  },
  {
    "loss": 2.9665,
    "grad_norm": 0.7636786699295044,
    "learning_rate": 0.0001700840336134454,
    "epoch": 0.41804180418041803,
    "step": 95
  },
  {
    "loss": 2.9025,
    "grad_norm": 0.7474446296691895,
    "learning_rate": 0.00016974789915966387,
    "epoch": 0.42244224422442245,
    "step": 96
  },
  {
    "loss": 2.9617,
    "grad_norm": 0.7604408860206604,
    "learning_rate": 0.00016941176470588237,
    "epoch": 0.42684268426842686,
    "step": 97
  },
  {
    "loss": 2.9163,
    "grad_norm": 0.7007436752319336,
    "learning_rate": 0.00016907563025210084,
    "epoch": 0.4312431243124312,
    "step": 98
  },
  {
    "loss": 3.058,
    "grad_norm": 0.6644778847694397,
    "learning_rate": 0.00016873949579831934,
    "epoch": 0.43564356435643564,
    "step": 99
  },
  {
    "loss": 3.2857,
    "grad_norm": 0.7980109453201294,
    "learning_rate": 0.00016840336134453784,
    "epoch": 0.44004400440044006,
    "step": 100
  },
  {
    "loss": 2.9902,
    "grad_norm": 0.5852857828140259,
    "learning_rate": 0.0001680672268907563,
    "epoch": 0.4444444444444444,
    "step": 101
  },
  {
    "loss": 3.0435,
    "grad_norm": 0.8208147287368774,
    "learning_rate": 0.0001677310924369748,
    "epoch": 0.44884488448844884,
    "step": 102
  },
  {
    "loss": 3.0583,
    "grad_norm": 0.8669224977493286,
    "learning_rate": 0.00016739495798319328,
    "epoch": 0.45324532453245325,
    "step": 103
  },
  {
    "loss": 2.9714,
    "grad_norm": 0.7763462066650391,
    "learning_rate": 0.00016705882352941178,
    "epoch": 0.45764576457645767,
    "step": 104
  },
  {
    "loss": 2.9469,
    "grad_norm": 0.669762372970581,
    "learning_rate": 0.00016672268907563028,
    "epoch": 0.46204620462046203,
    "step": 105
  },
  {
    "loss": 3.1036,
    "grad_norm": 0.668157696723938,
    "learning_rate": 0.00016638655462184875,
    "epoch": 0.46644664466446645,
    "step": 106
  },
  {
    "loss": 3.0055,
    "grad_norm": 0.7309333086013794,
    "learning_rate": 0.00016605042016806725,
    "epoch": 0.47084708470847086,
    "step": 107
  },
  {
    "loss": 2.9382,
    "grad_norm": 0.7091376781463623,
    "learning_rate": 0.00016571428571428575,
    "epoch": 0.4752475247524752,
    "step": 108
  },
  {
    "loss": 2.9763,
    "grad_norm": 0.7049872279167175,
    "learning_rate": 0.0001653781512605042,
    "epoch": 0.47964796479647964,
    "step": 109
  },
  {
    "loss": 3.0727,
    "grad_norm": 0.5967928767204285,
    "learning_rate": 0.0001650420168067227,
    "epoch": 0.48404840484048406,
    "step": 110
  },
  {
    "loss": 3.215,
    "grad_norm": 0.6273902058601379,
    "learning_rate": 0.0001647058823529412,
    "epoch": 0.4884488448844885,
    "step": 111
  },
  {
    "loss": 2.8692,
    "grad_norm": 0.5986081957817078,
    "learning_rate": 0.00016436974789915966,
    "epoch": 0.49284928492849284,
    "step": 112
  },
  {
    "loss": 2.9924,
    "grad_norm": 0.7813271284103394,
    "learning_rate": 0.00016403361344537816,
    "epoch": 0.49724972497249725,
    "step": 113
  },
  {
    "loss": 3.039,
    "grad_norm": 0.6678581833839417,
    "learning_rate": 0.00016369747899159663,
    "epoch": 0.5016501650165016,
    "step": 114
  },
  {
    "loss": 3.0692,
    "grad_norm": 0.6795379519462585,
    "learning_rate": 0.00016336134453781513,
    "epoch": 0.506050605060506,
    "step": 115
  },
  {
    "loss": 2.9744,
    "grad_norm": 0.7079314589500427,
    "learning_rate": 0.00016302521008403363,
    "epoch": 0.5104510451045104,
    "step": 116
  },
  {
    "loss": 2.919,
    "grad_norm": 0.6996665596961975,
    "learning_rate": 0.0001626890756302521,
    "epoch": 0.5148514851485149,
    "step": 117
  },
  {
    "loss": 2.9791,
    "grad_norm": 0.6860982775688171,
    "learning_rate": 0.0001623529411764706,
    "epoch": 0.5192519251925193,
    "step": 118
  },
  {
    "loss": 3.0199,
    "grad_norm": 0.7723775506019592,
    "learning_rate": 0.00016201680672268907,
    "epoch": 0.5236523652365237,
    "step": 119
  },
  {
    "loss": 2.9965,
    "grad_norm": 0.6709608435630798,
    "learning_rate": 0.00016168067226890757,
    "epoch": 0.528052805280528,
    "step": 120
  },
  {
    "loss": 3.089,
    "grad_norm": 0.748449444770813,
    "learning_rate": 0.00016134453781512607,
    "epoch": 0.5324532453245324,
    "step": 121
  },
  {
    "loss": 2.9283,
    "grad_norm": 0.6070863604545593,
    "learning_rate": 0.00016100840336134454,
    "epoch": 0.5368536853685368,
    "step": 122
  },
  {
    "loss": 2.9413,
    "grad_norm": 0.7803452610969543,
    "learning_rate": 0.00016067226890756304,
    "epoch": 0.5412541254125413,
    "step": 123
  },
  {
    "loss": 2.9877,
    "grad_norm": 0.7799443006515503,
    "learning_rate": 0.00016033613445378154,
    "epoch": 0.5456545654565457,
    "step": 124
  },
  {
    "loss": 3.143,
    "grad_norm": 0.5923717021942139,
    "learning_rate": 0.00016,
    "epoch": 0.5500550055005501,
    "step": 125
  },
  {
    "loss": 2.8154,
    "grad_norm": 0.8193175792694092,
    "learning_rate": 0.0001596638655462185,
    "epoch": 0.5544554455445545,
    "step": 126
  },
  {
    "loss": 2.9841,
    "grad_norm": 0.7774813771247864,
    "learning_rate": 0.00015932773109243698,
    "epoch": 0.5588558855885588,
    "step": 127
  },
  {
    "loss": 2.9494,
    "grad_norm": 0.6742253303527832,
    "learning_rate": 0.00015899159663865546,
    "epoch": 0.5632563256325632,
    "step": 128
  },
  {
    "loss": 2.9388,
    "grad_norm": 0.7644140720367432,
    "learning_rate": 0.00015865546218487396,
    "epoch": 0.5676567656765676,
    "step": 129
  },
  {
    "loss": 2.9326,
    "grad_norm": 0.6771105527877808,
    "learning_rate": 0.00015831932773109243,
    "epoch": 0.5720572057205721,
    "step": 130
  },
  {
    "loss": 2.8364,
    "grad_norm": 0.6658768653869629,
    "learning_rate": 0.00015798319327731093,
    "epoch": 0.5764576457645765,
    "step": 131
  },
  {
    "loss": 3.1854,
    "grad_norm": 0.780129611492157,
    "learning_rate": 0.00015764705882352943,
    "epoch": 0.5808580858085809,
    "step": 132
  },
  {
    "loss": 3.0903,
    "grad_norm": 0.6752039194107056,
    "learning_rate": 0.0001573109243697479,
    "epoch": 0.5852585258525853,
    "step": 133
  },
  {
    "loss": 2.8874,
    "grad_norm": 0.8153931498527527,
    "learning_rate": 0.0001569747899159664,
    "epoch": 0.5896589658965896,
    "step": 134
  },
  {
    "loss": 3.1068,
    "grad_norm": 0.5867688655853271,
    "learning_rate": 0.00015663865546218487,
    "epoch": 0.594059405940594,
    "step": 135
  },
  {
    "loss": 2.9203,
    "grad_norm": 0.5703863501548767,
    "learning_rate": 0.00015630252100840337,
    "epoch": 0.5984598459845984,
    "step": 136
  },
  {
    "loss": 3.1807,
    "grad_norm": 0.5827840566635132,
    "learning_rate": 0.00015596638655462187,
    "epoch": 0.6028602860286029,
    "step": 137
  },
  {
    "loss": 3.0541,
    "grad_norm": 0.7350829839706421,
    "learning_rate": 0.00015563025210084034,
    "epoch": 0.6072607260726073,
    "step": 138
  },
  {
    "loss": 3.0047,
    "grad_norm": 0.7629983425140381,
    "learning_rate": 0.00015529411764705884,
    "epoch": 0.6116611661166117,
    "step": 139
  },
  {
    "loss": 2.9062,
    "grad_norm": 0.7585829496383667,
    "learning_rate": 0.00015495798319327734,
    "epoch": 0.6160616061606161,
    "step": 140
  },
  {
    "loss": 3.0039,
    "grad_norm": 0.6470947861671448,
    "learning_rate": 0.0001546218487394958,
    "epoch": 0.6204620462046204,
    "step": 141
  },
  {
    "loss": 3.0708,
    "grad_norm": 0.6243334412574768,
    "learning_rate": 0.0001542857142857143,
    "epoch": 0.6248624862486248,
    "step": 142
  },
  {
    "loss": 3.1431,
    "grad_norm": 0.609472393989563,
    "learning_rate": 0.00015394957983193278,
    "epoch": 0.6292629262926293,
    "step": 143
  },
  {
    "loss": 2.9603,
    "grad_norm": 0.6995958089828491,
    "learning_rate": 0.00015361344537815128,
    "epoch": 0.6336633663366337,
    "step": 144
  },
  {
    "loss": 2.896,
    "grad_norm": 0.6906396746635437,
    "learning_rate": 0.00015327731092436978,
    "epoch": 0.6380638063806381,
    "step": 145
  },
  {
    "loss": 2.7246,
    "grad_norm": 0.6450381278991699,
    "learning_rate": 0.00015294117647058822,
    "epoch": 0.6424642464246425,
    "step": 146
  },
  {
    "loss": 2.93,
    "grad_norm": 0.6704791784286499,
    "learning_rate": 0.00015260504201680672,
    "epoch": 0.6468646864686468,
    "step": 147
  },
  {
    "loss": 3.0148,
    "grad_norm": 0.7459424138069153,
    "learning_rate": 0.00015226890756302522,
    "epoch": 0.6512651265126512,
    "step": 148
  },
  {
    "loss": 2.884,
    "grad_norm": 0.7109645009040833,
    "learning_rate": 0.0001519327731092437,
    "epoch": 0.6556655665566556,
    "step": 149
  },
  {
    "loss": 2.8671,
    "grad_norm": 0.7138137221336365,
    "learning_rate": 0.0001515966386554622,
    "epoch": 0.6600660066006601,
    "step": 150
  },
  {
    "loss": 3.0334,
    "grad_norm": 0.680840253829956,
    "learning_rate": 0.00015126050420168066,
    "epoch": 0.6644664466446645,
    "step": 151
  },
  {
    "loss": 2.8717,
    "grad_norm": 0.7647421956062317,
    "learning_rate": 0.00015092436974789916,
    "epoch": 0.6688668866886689,
    "step": 152
  },
  {
    "loss": 2.784,
    "grad_norm": 0.8269438743591309,
    "learning_rate": 0.00015058823529411766,
    "epoch": 0.6732673267326733,
    "step": 153
  },
  {
    "loss": 2.8968,
    "grad_norm": 0.689660370349884,
    "learning_rate": 0.00015025210084033613,
    "epoch": 0.6776677667766776,
    "step": 154
  },
  {
    "loss": 3.0607,
    "grad_norm": 0.7312266230583191,
    "learning_rate": 0.00014991596638655463,
    "epoch": 0.682068206820682,
    "step": 155
  },
  {
    "loss": 2.8815,
    "grad_norm": 0.7264052033424377,
    "learning_rate": 0.00014957983193277313,
    "epoch": 0.6864686468646864,
    "step": 156
  },
  {
    "loss": 3.0726,
    "grad_norm": 0.5841848254203796,
    "learning_rate": 0.0001492436974789916,
    "epoch": 0.6908690869086909,
    "step": 157
  },
  {
    "loss": 3.0225,
    "grad_norm": 0.6007042527198792,
    "learning_rate": 0.0001489075630252101,
    "epoch": 0.6952695269526953,
    "step": 158
  },
  {
    "loss": 2.8694,
    "grad_norm": 0.7096611261367798,
    "learning_rate": 0.00014857142857142857,
    "epoch": 0.6996699669966997,
    "step": 159
  },
  {
    "loss": 3.0202,
    "grad_norm": 0.6716635227203369,
    "learning_rate": 0.00014823529411764707,
    "epoch": 0.7040704070407041,
    "step": 160
  },
  {
    "loss": 3.037,
    "grad_norm": 0.7232773303985596,
    "learning_rate": 0.00014789915966386557,
    "epoch": 0.7084708470847084,
    "step": 161
  },
  {
    "loss": 3.1098,
    "grad_norm": 0.679416298866272,
    "learning_rate": 0.00014756302521008404,
    "epoch": 0.7128712871287128,
    "step": 162
  },
  {
    "loss": 2.8937,
    "grad_norm": 0.5967116951942444,
    "learning_rate": 0.00014722689075630254,
    "epoch": 0.7172717271727173,
    "step": 163
  },
  {
    "loss": 2.9166,
    "grad_norm": 0.7145591378211975,
    "learning_rate": 0.00014689075630252101,
    "epoch": 0.7216721672167217,
    "step": 164
  },
  {
    "loss": 2.915,
    "grad_norm": 0.6492558121681213,
    "learning_rate": 0.0001465546218487395,
    "epoch": 0.7260726072607261,
    "step": 165
  },
  {
    "loss": 2.8978,
    "grad_norm": 0.7793040871620178,
    "learning_rate": 0.00014621848739495799,
    "epoch": 0.7304730473047305,
    "step": 166
  },
  {
    "loss": 2.9861,
    "grad_norm": 0.8703455924987793,
    "learning_rate": 0.00014588235294117646,
    "epoch": 0.7348734873487349,
    "step": 167
  },
  {
    "loss": 2.9393,
    "grad_norm": 0.6528028249740601,
    "learning_rate": 0.00014554621848739496,
    "epoch": 0.7392739273927392,
    "step": 168
  },
  {
    "loss": 2.5959,
    "grad_norm": 0.8551297783851624,
    "learning_rate": 0.00014521008403361346,
    "epoch": 0.7436743674367436,
    "step": 169
  },
  {
    "loss": 3.1206,
    "grad_norm": 0.6316116452217102,
    "learning_rate": 0.00014487394957983193,
    "epoch": 0.7480748074807481,
    "step": 170
  },
  {
    "loss": 2.949,
    "grad_norm": 0.636849045753479,
    "learning_rate": 0.00014453781512605043,
    "epoch": 0.7524752475247525,
    "step": 171
  },
  {
    "loss": 2.9091,
    "grad_norm": 0.7957901358604431,
    "learning_rate": 0.00014420168067226893,
    "epoch": 0.7568756875687569,
    "step": 172
  },
  {
    "loss": 3.0621,
    "grad_norm": 0.7142845392227173,
    "learning_rate": 0.0001438655462184874,
    "epoch": 0.7612761276127613,
    "step": 173
  },
  {
    "loss": 2.9746,
    "grad_norm": 0.7426927089691162,
    "learning_rate": 0.0001435294117647059,
    "epoch": 0.7656765676567657,
    "step": 174
  },
  {
    "loss": 3.0129,
    "grad_norm": 0.7043387293815613,
    "learning_rate": 0.00014319327731092437,
    "epoch": 0.77007700770077,
    "step": 175
  },
  {
    "loss": 2.8667,
    "grad_norm": 0.5801270008087158,
    "learning_rate": 0.00014285714285714287,
    "epoch": 0.7744774477447744,
    "step": 176
  },
  {
    "loss": 3.0576,
    "grad_norm": 0.55449378490448,
    "learning_rate": 0.00014252100840336137,
    "epoch": 0.7788778877887789,
    "step": 177
  },
  {
    "loss": 2.933,
    "grad_norm": 0.6261522173881531,
    "learning_rate": 0.00014218487394957984,
    "epoch": 0.7832783278327833,
    "step": 178
  },
  {
    "loss": 3.1347,
    "grad_norm": 0.9197235703468323,
    "learning_rate": 0.00014184873949579834,
    "epoch": 0.7876787678767877,
    "step": 179
  },
  {
    "loss": 3.2157,
    "grad_norm": 0.6767386794090271,
    "learning_rate": 0.0001415126050420168,
    "epoch": 0.7920792079207921,
    "step": 180
  },
  {
    "loss": 2.9861,
    "grad_norm": 0.761611819267273,
    "learning_rate": 0.0001411764705882353,
    "epoch": 0.7964796479647965,
    "step": 181
  },
  {
    "loss": 2.8725,
    "grad_norm": 0.585496187210083,
    "learning_rate": 0.0001408403361344538,
    "epoch": 0.8008800880088008,
    "step": 182
  },
  {
    "loss": 2.9152,
    "grad_norm": 0.7395204305648804,
    "learning_rate": 0.00014050420168067225,
    "epoch": 0.8052805280528053,
    "step": 183
  },
  {
    "loss": 2.9115,
    "grad_norm": 0.7273691296577454,
    "learning_rate": 0.00014016806722689075,
    "epoch": 0.8096809680968097,
    "step": 184
  },
  {
    "loss": 3.0134,
    "grad_norm": 0.6849024295806885,
    "learning_rate": 0.00013983193277310925,
    "epoch": 0.8140814081408141,
    "step": 185
  },
  {
    "loss": 2.9294,
    "grad_norm": 0.706699013710022,
    "learning_rate": 0.00013949579831932772,
    "epoch": 0.8184818481848185,
    "step": 186
  },
  {
    "loss": 2.883,
    "grad_norm": 0.6451299786567688,
    "learning_rate": 0.00013915966386554622,
    "epoch": 0.8228822882288229,
    "step": 187
  },
  {
    "loss": 2.964,
    "grad_norm": 0.8670132160186768,
    "learning_rate": 0.00013882352941176472,
    "epoch": 0.8272827282728272,
    "step": 188
  },
  {
    "loss": 3.2474,
    "grad_norm": 0.7519623041152954,
    "learning_rate": 0.0001384873949579832,
    "epoch": 0.8316831683168316,
    "step": 189
  },
  {
    "loss": 2.8032,
    "grad_norm": 0.6660257577896118,
    "learning_rate": 0.0001381512605042017,
    "epoch": 0.8360836083608361,
    "step": 190
  },
  {
    "loss": 2.9708,
    "grad_norm": 0.6643628478050232,
    "learning_rate": 0.00013781512605042016,
    "epoch": 0.8404840484048405,
    "step": 191
  },
  {
    "loss": 2.8745,
    "grad_norm": 0.7653730511665344,
    "learning_rate": 0.00013747899159663866,
    "epoch": 0.8448844884488449,
    "step": 192
  },
  {
    "loss": 3.0937,
    "grad_norm": 0.6587067246437073,
    "learning_rate": 0.00013714285714285716,
    "epoch": 0.8492849284928493,
    "step": 193
  },
  {
    "loss": 2.8713,
    "grad_norm": 0.68438720703125,
    "learning_rate": 0.00013680672268907563,
    "epoch": 0.8536853685368537,
    "step": 194
  },
  {
    "loss": 3.0608,
    "grad_norm": 0.6198664307594299,
    "learning_rate": 0.00013647058823529413,
    "epoch": 0.858085808580858,
    "step": 195
  },
  {
    "loss": 3.0413,
    "grad_norm": 0.6161460280418396,
    "learning_rate": 0.0001361344537815126,
    "epoch": 0.8624862486248625,
    "step": 196
  },
  {
    "loss": 3.0017,
    "grad_norm": 0.6942824125289917,
    "learning_rate": 0.0001357983193277311,
    "epoch": 0.8668866886688669,
    "step": 197
  },
  {
    "loss": 2.9702,
    "grad_norm": 0.6709293127059937,
    "learning_rate": 0.0001354621848739496,
    "epoch": 0.8712871287128713,
    "step": 198
  },
  {
    "loss": 3.1929,
    "grad_norm": 0.6848558783531189,
    "learning_rate": 0.00013512605042016807,
    "epoch": 0.8756875687568757,
    "step": 199
  },
  {
    "loss": 2.9344,
    "grad_norm": 0.7276372909545898,
    "learning_rate": 0.00013478991596638657,
    "epoch": 0.8800880088008801,
    "step": 200
  },
  {
    "loss": 2.7656,
    "grad_norm": 0.6785969734191895,
    "learning_rate": 0.00013445378151260507,
    "epoch": 0.8844884488448845,
    "step": 201
  },
  {
    "loss": 2.8384,
    "grad_norm": 0.7738552093505859,
    "learning_rate": 0.00013411764705882352,
    "epoch": 0.8888888888888888,
    "step": 202
  },
  {
    "loss": 3.0621,
    "grad_norm": 0.7698157429695129,
    "learning_rate": 0.00013378151260504202,
    "epoch": 0.8932893289328933,
    "step": 203
  },
  {
    "loss": 2.7766,
    "grad_norm": 0.6709082126617432,
    "learning_rate": 0.00013344537815126052,
    "epoch": 0.8976897689768977,
    "step": 204
  },
  {
    "loss": 2.8288,
    "grad_norm": 0.7265191674232483,
    "learning_rate": 0.000133109243697479,
    "epoch": 0.9020902090209021,
    "step": 205
  },
  {
    "loss": 2.7463,
    "grad_norm": 0.6473321318626404,
    "learning_rate": 0.0001327731092436975,
    "epoch": 0.9064906490649065,
    "step": 206
  },
  {
    "loss": 2.8657,
    "grad_norm": 0.7615755796432495,
    "learning_rate": 0.00013243697478991596,
    "epoch": 0.9108910891089109,
    "step": 207
  },
  {
    "loss": 2.9944,
    "grad_norm": 0.8228421211242676,
    "learning_rate": 0.00013210084033613446,
    "epoch": 0.9152915291529153,
    "step": 208
  },
  {
    "loss": 2.952,
    "grad_norm": 0.7714436054229736,
    "learning_rate": 0.00013176470588235296,
    "epoch": 0.9196919691969196,
    "step": 209
  },
  {
    "loss": 3.0064,
    "grad_norm": 0.7044273614883423,
    "learning_rate": 0.00013142857142857143,
    "epoch": 0.9240924092409241,
    "step": 210
  },
  {
    "loss": 2.7952,
    "grad_norm": 0.6884092092514038,
    "learning_rate": 0.00013109243697478993,
    "epoch": 0.9284928492849285,
    "step": 211
  },
  {
    "loss": 2.887,
    "grad_norm": 0.6168733835220337,
    "learning_rate": 0.0001307563025210084,
    "epoch": 0.9328932893289329,
    "step": 212
  },
  {
    "loss": 3.0854,
    "grad_norm": 0.8101485371589661,
    "learning_rate": 0.0001304201680672269,
    "epoch": 0.9372937293729373,
    "step": 213
  },
  {
    "loss": 2.8523,
    "grad_norm": 0.666972279548645,
    "learning_rate": 0.0001300840336134454,
    "epoch": 0.9416941694169417,
    "step": 214
  },
  {
    "loss": 2.9381,
    "grad_norm": 0.6869277954101562,
    "learning_rate": 0.00012974789915966387,
    "epoch": 0.9460946094609461,
    "step": 215
  },
  {
    "loss": 2.8239,
    "grad_norm": 0.6702711582183838,
    "learning_rate": 0.00012941176470588237,
    "epoch": 0.9504950495049505,
    "step": 216
  },
  {
    "loss": 2.7278,
    "grad_norm": 0.6390085816383362,
    "learning_rate": 0.00012907563025210087,
    "epoch": 0.9548954895489549,
    "step": 217
  },
  {
    "loss": 2.7455,
    "grad_norm": 0.6975240707397461,
    "learning_rate": 0.00012873949579831934,
    "epoch": 0.9592959295929593,
    "step": 218
  },
  {
    "loss": 2.9181,
    "grad_norm": 0.6423905491828918,
    "learning_rate": 0.00012840336134453784,
    "epoch": 0.9636963696369637,
    "step": 219
  },
  {
    "loss": 3.0692,
    "grad_norm": 0.6693125367164612,
    "learning_rate": 0.0001280672268907563,
    "epoch": 0.9680968096809681,
    "step": 220
  },
  {
    "loss": 3.0559,
    "grad_norm": 0.6598517298698425,
    "learning_rate": 0.00012773109243697478,
    "epoch": 0.9724972497249725,
    "step": 221
  },
  {
    "loss": 2.6878,
    "grad_norm": 0.8227919936180115,
    "learning_rate": 0.00012739495798319328,
    "epoch": 0.976897689768977,
    "step": 222
  },
  {
    "loss": 2.8967,
    "grad_norm": 0.7420979738235474,
    "learning_rate": 0.00012705882352941175,
    "epoch": 0.9812981298129813,
    "step": 223
  },
  {
    "loss": 2.8265,
    "grad_norm": 0.6978186964988708,
    "learning_rate": 0.00012672268907563025,
    "epoch": 0.9856985698569857,
    "step": 224
  },
  {
    "loss": 2.9023,
    "grad_norm": 0.7650659084320068,
    "learning_rate": 0.00012638655462184875,
    "epoch": 0.9900990099009901,
    "step": 225
  },
  {
    "loss": 2.8665,
    "grad_norm": 0.7756250500679016,
    "learning_rate": 0.00012605042016806722,
    "epoch": 0.9944994499449945,
    "step": 226
  },
  {
    "loss": 3.0364,
    "grad_norm": 0.7013545036315918,
    "learning_rate": 0.00012571428571428572,
    "epoch": 0.9988998899889989,
    "step": 227
  },
  {
    "loss": 3.101,
    "grad_norm": 1.6084437370300293,
    "learning_rate": 0.0001253781512605042,
    "epoch": 1.0,
    "step": 228
  },
  {
    "loss": 2.8527,
    "grad_norm": 0.7662386298179626,
    "learning_rate": 0.0001250420168067227,
    "epoch": 1.0044004400440043,
    "step": 229
  },
  {
    "loss": 2.61,
    "grad_norm": 0.7565757632255554,
    "learning_rate": 0.0001247058823529412,
    "epoch": 1.0088008800880088,
    "step": 230
  },
  {
    "loss": 2.7043,
    "grad_norm": 0.783077597618103,
    "learning_rate": 0.00012436974789915966,
    "epoch": 1.0132013201320131,
    "step": 231
  },
  {
    "loss": 2.8448,
    "grad_norm": 0.8650872707366943,
    "learning_rate": 0.00012403361344537816,
    "epoch": 1.0176017601760177,
    "step": 232
  },
  {
    "loss": 2.9989,
    "grad_norm": 0.7712415456771851,
    "learning_rate": 0.00012369747899159666,
    "epoch": 1.022002200220022,
    "step": 233
  },
  {
    "loss": 2.8802,
    "grad_norm": 0.7040818929672241,
    "learning_rate": 0.00012336134453781513,
    "epoch": 1.0264026402640265,
    "step": 234
  },
  {
    "loss": 2.8743,
    "grad_norm": 0.6817421913146973,
    "learning_rate": 0.00012302521008403363,
    "epoch": 1.0308030803080308,
    "step": 235
  },
  {
    "loss": 2.671,
    "grad_norm": 0.8587483167648315,
    "learning_rate": 0.0001226890756302521,
    "epoch": 1.0352035203520351,
    "step": 236
  },
  {
    "loss": 2.6755,
    "grad_norm": 0.7261464595794678,
    "learning_rate": 0.0001223529411764706,
    "epoch": 1.0396039603960396,
    "step": 237
  },
  {
    "loss": 2.937,
    "grad_norm": 0.7399464249610901,
    "learning_rate": 0.00012201680672268909,
    "epoch": 1.044004400440044,
    "step": 238
  },
  {
    "loss": 2.5278,
    "grad_norm": 0.6324896812438965,
    "learning_rate": 0.00012168067226890756,
    "epoch": 1.0484048404840485,
    "step": 239
  },
  {
    "loss": 2.8728,
    "grad_norm": 0.6579781770706177,
    "learning_rate": 0.00012134453781512605,
    "epoch": 1.0528052805280528,
    "step": 240
  },
  {
    "loss": 2.8381,
    "grad_norm": 0.7129287719726562,
    "learning_rate": 0.00012100840336134453,
    "epoch": 1.0572057205720573,
    "step": 241
  },
  {
    "loss": 2.7788,
    "grad_norm": 0.673542320728302,
    "learning_rate": 0.00012067226890756302,
    "epoch": 1.0616061606160616,
    "step": 242
  },
  {
    "loss": 2.7833,
    "grad_norm": 0.7305843234062195,
    "learning_rate": 0.00012033613445378152,
    "epoch": 1.066006600660066,
    "step": 243
  },
  {
    "loss": 2.9847,
    "grad_norm": 0.8443084359169006,
    "learning_rate": 0.00012,
    "epoch": 1.0704070407040704,
    "step": 244
  },
  {
    "loss": 2.6683,
    "grad_norm": 0.5959243178367615,
    "learning_rate": 0.00011966386554621849,
    "epoch": 1.0748074807480748,
    "step": 245
  },
  {
    "loss": 2.7424,
    "grad_norm": 0.6954940557479858,
    "learning_rate": 0.00011932773109243697,
    "epoch": 1.0792079207920793,
    "step": 246
  },
  {
    "loss": 2.9174,
    "grad_norm": 0.8032038807868958,
    "learning_rate": 0.00011899159663865547,
    "epoch": 1.0836083608360836,
    "step": 247
  },
  {
    "loss": 2.9972,
    "grad_norm": 0.7659370303153992,
    "learning_rate": 0.00011865546218487396,
    "epoch": 1.0880088008800881,
    "step": 248
  },
  {
    "loss": 2.8867,
    "grad_norm": 0.7610740661621094,
    "learning_rate": 0.00011831932773109244,
    "epoch": 1.0924092409240924,
    "step": 249
  },
  {
    "loss": 3.0576,
    "grad_norm": 0.668076753616333,
    "learning_rate": 0.00011798319327731093,
    "epoch": 1.0968096809680967,
    "step": 250
  },
  {
    "loss": 2.7441,
    "grad_norm": 0.6698362827301025,
    "learning_rate": 0.00011764705882352942,
    "epoch": 1.1012101210121013,
    "step": 251
  },
  {
    "loss": 2.8286,
    "grad_norm": 0.6915496587753296,
    "learning_rate": 0.00011731092436974791,
    "epoch": 1.1056105610561056,
    "step": 252
  },
  {
    "loss": 2.5839,
    "grad_norm": 0.8808819651603699,
    "learning_rate": 0.0001169747899159664,
    "epoch": 1.11001100110011,
    "step": 253
  },
  {
    "loss": 2.7496,
    "grad_norm": 0.8054481744766235,
    "learning_rate": 0.00011663865546218489,
    "epoch": 1.1144114411441144,
    "step": 254
  },
  {
    "loss": 2.7742,
    "grad_norm": 0.7563315629959106,
    "learning_rate": 0.00011630252100840337,
    "epoch": 1.118811881188119,
    "step": 255
  },
  {
    "loss": 2.912,
    "grad_norm": 0.7418895959854126,
    "learning_rate": 0.00011596638655462187,
    "epoch": 1.1232123212321232,
    "step": 256
  },
  {
    "loss": 2.7041,
    "grad_norm": 0.7452741861343384,
    "learning_rate": 0.00011563025210084036,
    "epoch": 1.1276127612761275,
    "step": 257
  },
  {
    "loss": 2.929,
    "grad_norm": 0.825771689414978,
    "learning_rate": 0.00011529411764705881,
    "epoch": 1.132013201320132,
    "step": 258
  },
  {
    "loss": 2.8547,
    "grad_norm": 0.7729243636131287,
    "learning_rate": 0.00011495798319327731,
    "epoch": 1.1364136413641364,
    "step": 259
  },
  {
    "loss": 2.621,
    "grad_norm": 0.7414449453353882,
    "learning_rate": 0.0001146218487394958,
    "epoch": 1.140814081408141,
    "step": 260
  },
  {
    "loss": 2.9306,
    "grad_norm": 0.676031768321991,
    "learning_rate": 0.00011428571428571428,
    "epoch": 1.1452145214521452,
    "step": 261
  },
  {
    "loss": 2.6094,
    "grad_norm": 0.7163274884223938,
    "learning_rate": 0.00011394957983193277,
    "epoch": 1.1496149614961495,
    "step": 262
  },
  {
    "loss": 2.7666,
    "grad_norm": 0.7646228075027466,
    "learning_rate": 0.00011361344537815127,
    "epoch": 1.154015401540154,
    "step": 263
  },
  {
    "loss": 2.6092,
    "grad_norm": 0.7550088763237,
    "learning_rate": 0.00011327731092436975,
    "epoch": 1.1584158415841583,
    "step": 264
  },
  {
    "loss": 2.8342,
    "grad_norm": 1.0377742052078247,
    "learning_rate": 0.00011294117647058824,
    "epoch": 1.1628162816281629,
    "step": 265
  },
  {
    "loss": 3.0103,
    "grad_norm": 0.8329241871833801,
    "learning_rate": 0.00011260504201680672,
    "epoch": 1.1672167216721672,
    "step": 266
  },
  {
    "loss": 2.678,
    "grad_norm": 0.7749847769737244,
    "learning_rate": 0.00011226890756302521,
    "epoch": 1.1716171617161717,
    "step": 267
  },
  {
    "loss": 2.7998,
    "grad_norm": 0.7823947668075562,
    "learning_rate": 0.00011193277310924371,
    "epoch": 1.176017601760176,
    "step": 268
  },
  {
    "loss": 2.8065,
    "grad_norm": 0.8494422435760498,
    "learning_rate": 0.0001115966386554622,
    "epoch": 1.1804180418041805,
    "step": 269
  },
  {
    "loss": 2.9102,
    "grad_norm": 0.882502019405365,
    "learning_rate": 0.00011126050420168068,
    "epoch": 1.1848184818481848,
    "step": 270
  },
  {
    "loss": 3.029,
    "grad_norm": 0.7767595052719116,
    "learning_rate": 0.00011092436974789917,
    "epoch": 1.1892189218921891,
    "step": 271
  },
  {
    "loss": 2.8988,
    "grad_norm": 0.8846942186355591,
    "learning_rate": 0.00011058823529411766,
    "epoch": 1.1936193619361937,
    "step": 272
  },
  {
    "loss": 3.0683,
    "grad_norm": 0.7693215608596802,
    "learning_rate": 0.00011025210084033615,
    "epoch": 1.198019801980198,
    "step": 273
  },
  {
    "loss": 2.913,
    "grad_norm": 0.6702226996421814,
    "learning_rate": 0.00010991596638655464,
    "epoch": 1.2024202420242025,
    "step": 274
  },
  {
    "loss": 2.8464,
    "grad_norm": 0.7044078707695007,
    "learning_rate": 0.00010957983193277312,
    "epoch": 1.2068206820682068,
    "step": 275
  },
  {
    "loss": 2.8428,
    "grad_norm": 0.7296264171600342,
    "learning_rate": 0.00010924369747899159,
    "epoch": 1.2112211221122111,
    "step": 276
  },
  {
    "loss": 3.0368,
    "grad_norm": 0.8733790516853333,
    "learning_rate": 0.00010890756302521008,
    "epoch": 1.2156215621562156,
    "step": 277
  },
  {
    "loss": 2.8333,
    "grad_norm": 0.8667478561401367,
    "learning_rate": 0.00010857142857142856,
    "epoch": 1.22002200220022,
    "step": 278
  },
  {
    "loss": 2.8099,
    "grad_norm": 0.9368005394935608,
    "learning_rate": 0.00010823529411764706,
    "epoch": 1.2244224422442245,
    "step": 279
  },
  {
    "loss": 2.9148,
    "grad_norm": 0.8038969039916992,
    "learning_rate": 0.00010789915966386555,
    "epoch": 1.2288228822882288,
    "step": 280
  },
  {
    "loss": 2.7623,
    "grad_norm": 0.8517576456069946,
    "learning_rate": 0.00010756302521008403,
    "epoch": 1.2332233223322333,
    "step": 281
  },
  {
    "loss": 2.9346,
    "grad_norm": 0.7010291814804077,
    "learning_rate": 0.00010722689075630252,
    "epoch": 1.2376237623762376,
    "step": 282
  },
  {
    "loss": 2.8506,
    "grad_norm": 0.6739490628242493,
    "learning_rate": 0.000106890756302521,
    "epoch": 1.2420242024202421,
    "step": 283
  },
  {
    "loss": 2.8518,
    "grad_norm": 0.7351922988891602,
    "learning_rate": 0.0001065546218487395,
    "epoch": 1.2464246424642464,
    "step": 284
  },
  {
    "loss": 2.9439,
    "grad_norm": 0.8144029974937439,
    "learning_rate": 0.00010621848739495799,
    "epoch": 1.2508250825082508,
    "step": 285
  },
  {
    "loss": 2.7434,
    "grad_norm": 0.8728498816490173,
    "learning_rate": 0.00010588235294117647,
    "epoch": 1.2552255225522553,
    "step": 286
  },
  {
    "loss": 2.8159,
    "grad_norm": 0.7144551277160645,
    "learning_rate": 0.00010554621848739496,
    "epoch": 1.2596259625962596,
    "step": 287
  },
  {
    "loss": 2.6458,
    "grad_norm": 0.8118088841438293,
    "learning_rate": 0.00010521008403361346,
    "epoch": 1.2640264026402641,
    "step": 288
  },
  {
    "loss": 2.7245,
    "grad_norm": 0.7399786114692688,
    "learning_rate": 0.00010487394957983194,
    "epoch": 1.2684268426842684,
    "step": 289
  },
  {
    "loss": 2.5622,
    "grad_norm": 0.8864490985870361,
    "learning_rate": 0.00010453781512605043,
    "epoch": 1.2728272827282727,
    "step": 290
  },
  {
    "loss": 2.7491,
    "grad_norm": 0.8031533360481262,
    "learning_rate": 0.00010420168067226892,
    "epoch": 1.2772277227722773,
    "step": 291
  },
  {
    "loss": 2.8848,
    "grad_norm": 0.6454015374183655,
    "learning_rate": 0.00010386554621848741,
    "epoch": 1.2816281628162816,
    "step": 292
  },
  {
    "loss": 2.7133,
    "grad_norm": 0.7323333621025085,
    "learning_rate": 0.0001035294117647059,
    "epoch": 1.286028602860286,
    "step": 293
  },
  {
    "loss": 2.504,
    "grad_norm": 0.8554489016532898,
    "learning_rate": 0.00010319327731092439,
    "epoch": 1.2904290429042904,
    "step": 294
  },
  {
    "loss": 2.7634,
    "grad_norm": 0.7735021710395813,
    "learning_rate": 0.00010285714285714286,
    "epoch": 1.2948294829482947,
    "step": 295
  },
  {
    "loss": 2.6498,
    "grad_norm": 0.7427827715873718,
    "learning_rate": 0.00010252100840336134,
    "epoch": 1.2992299229922992,
    "step": 296
  },
  {
    "loss": 2.9414,
    "grad_norm": 0.989022433757782,
    "learning_rate": 0.00010218487394957983,
    "epoch": 1.3036303630363038,
    "step": 297
  },
  {
    "loss": 3.0303,
    "grad_norm": 0.6871088147163391,
    "learning_rate": 0.00010184873949579831,
    "epoch": 1.308030803080308,
    "step": 298
  },
  {
    "loss": 2.6788,
    "grad_norm": 1.0564645528793335,
    "learning_rate": 0.0001015126050420168,
    "epoch": 1.3124312431243124,
    "step": 299
  },
  {
    "loss": 2.8901,
    "grad_norm": 0.7358714938163757,
    "learning_rate": 0.0001011764705882353,
    "epoch": 1.316831683168317,
    "step": 300
  },
  {
    "loss": 2.8357,
    "grad_norm": 0.7772088646888733,
    "learning_rate": 0.00010084033613445378,
    "epoch": 1.3212321232123212,
    "step": 301
  },
  {
    "loss": 2.8765,
    "grad_norm": 0.8047568798065186,
    "learning_rate": 0.00010050420168067227,
    "epoch": 1.3256325632563257,
    "step": 302
  },
  {
    "loss": 2.621,
    "grad_norm": 0.9940956234931946,
    "learning_rate": 0.00010016806722689076,
    "epoch": 1.33003300330033,
    "step": 303
  },
  {
    "loss": 2.744,
    "grad_norm": 0.9237272143363953,
    "learning_rate": 9.983193277310925e-05,
    "epoch": 1.3344334433443343,
    "step": 304
  },
  {
    "loss": 2.7599,
    "grad_norm": 0.7126286029815674,
    "learning_rate": 9.949579831932774e-05,
    "epoch": 1.3388338833883389,
    "step": 305
  },
  {
    "loss": 2.7175,
    "grad_norm": 0.8218382596969604,
    "learning_rate": 9.915966386554623e-05,
    "epoch": 1.3432343234323432,
    "step": 306
  },
  {
    "loss": 2.8501,
    "grad_norm": 0.8695494532585144,
    "learning_rate": 9.882352941176471e-05,
    "epoch": 1.3476347634763477,
    "step": 307
  },
  {
    "loss": 2.7255,
    "grad_norm": 0.8702430129051208,
    "learning_rate": 9.848739495798321e-05,
    "epoch": 1.352035203520352,
    "step": 308
  },
  {
    "loss": 2.8548,
    "grad_norm": 0.7757267355918884,
    "learning_rate": 9.815126050420168e-05,
    "epoch": 1.3564356435643563,
    "step": 309
  },
  {
    "loss": 2.9807,
    "grad_norm": 0.9067435264587402,
    "learning_rate": 9.781512605042017e-05,
    "epoch": 1.3608360836083608,
    "step": 310
  },
  {
    "loss": 2.9143,
    "grad_norm": 0.9032920002937317,
    "learning_rate": 9.747899159663865e-05,
    "epoch": 1.3652365236523654,
    "step": 311
  },
  {
    "loss": 2.6732,
    "grad_norm": 0.8679613471031189,
    "learning_rate": 9.714285714285715e-05,
    "epoch": 1.3696369636963697,
    "step": 312
  },
  {
    "loss": 3.0216,
    "grad_norm": 0.8338523507118225,
    "learning_rate": 9.680672268907564e-05,
    "epoch": 1.374037403740374,
    "step": 313
  },
  {
    "loss": 2.4661,
    "grad_norm": 0.8238485455513,
    "learning_rate": 9.647058823529412e-05,
    "epoch": 1.3784378437843785,
    "step": 314
  },
  {
    "loss": 2.8778,
    "grad_norm": 0.8495810031890869,
    "learning_rate": 9.613445378151261e-05,
    "epoch": 1.3828382838283828,
    "step": 315
  },
  {
    "loss": 2.6905,
    "grad_norm": 0.7733584046363831,
    "learning_rate": 9.579831932773111e-05,
    "epoch": 1.3872387238723873,
    "step": 316
  },
  {
    "loss": 2.8384,
    "grad_norm": 0.7327142953872681,
    "learning_rate": 9.546218487394959e-05,
    "epoch": 1.3916391639163916,
    "step": 317
  },
  {
    "loss": 2.9556,
    "grad_norm": 0.7731428146362305,
    "learning_rate": 9.512605042016806e-05,
    "epoch": 1.396039603960396,
    "step": 318
  },
  {
    "loss": 2.914,
    "grad_norm": 0.6998798251152039,
    "learning_rate": 9.478991596638655e-05,
    "epoch": 1.4004400440044005,
    "step": 319
  },
  {
    "loss": 2.9779,
    "grad_norm": 0.8231902718544006,
    "learning_rate": 9.445378151260505e-05,
    "epoch": 1.4048404840484048,
    "step": 320
  },
  {
    "loss": 2.9508,
    "grad_norm": 0.8382188677787781,
    "learning_rate": 9.411764705882353e-05,
    "epoch": 1.4092409240924093,
    "step": 321
  },
  {
    "loss": 2.6678,
    "grad_norm": 0.6451680660247803,
    "learning_rate": 9.378151260504202e-05,
    "epoch": 1.4136413641364136,
    "step": 322
  },
  {
    "loss": 2.6164,
    "grad_norm": 0.7457100749015808,
    "learning_rate": 9.34453781512605e-05,
    "epoch": 1.418041804180418,
    "step": 323
  },
  {
    "loss": 2.7537,
    "grad_norm": 0.9771395325660706,
    "learning_rate": 9.3109243697479e-05,
    "epoch": 1.4224422442244224,
    "step": 324
  },
  {
    "loss": 2.7501,
    "grad_norm": 0.7550742030143738,
    "learning_rate": 9.277310924369749e-05,
    "epoch": 1.426842684268427,
    "step": 325
  },
  {
    "loss": 2.9265,
    "grad_norm": 0.806721568107605,
    "learning_rate": 9.243697478991598e-05,
    "epoch": 1.4312431243124313,
    "step": 326
  },
  {
    "loss": 2.5486,
    "grad_norm": 0.8501200079917908,
    "learning_rate": 9.210084033613445e-05,
    "epoch": 1.4356435643564356,
    "step": 327
  },
  {
    "loss": 2.7753,
    "grad_norm": 0.7457987070083618,
    "learning_rate": 9.176470588235295e-05,
    "epoch": 1.4400440044004401,
    "step": 328
  },
  {
    "loss": 2.9241,
    "grad_norm": 0.835878312587738,
    "learning_rate": 9.142857142857143e-05,
    "epoch": 1.4444444444444444,
    "step": 329
  },
  {
    "loss": 2.888,
    "grad_norm": 0.7427805066108704,
    "learning_rate": 9.109243697478992e-05,
    "epoch": 1.448844884488449,
    "step": 330
  },
  {
    "loss": 3.1243,
    "grad_norm": 0.9473806619644165,
    "learning_rate": 9.07563025210084e-05,
    "epoch": 1.4532453245324533,
    "step": 331
  },
  {
    "loss": 2.7474,
    "grad_norm": 0.8001981973648071,
    "learning_rate": 9.04201680672269e-05,
    "epoch": 1.4576457645764576,
    "step": 332
  },
  {
    "loss": 3.0125,
    "grad_norm": 0.7757456302642822,
    "learning_rate": 9.008403361344539e-05,
    "epoch": 1.462046204620462,
    "step": 333
  },
  {
    "loss": 2.8041,
    "grad_norm": 0.7484933137893677,
    "learning_rate": 8.974789915966387e-05,
    "epoch": 1.4664466446644664,
    "step": 334
  },
  {
    "loss": 2.7602,
    "grad_norm": 0.817022442817688,
    "learning_rate": 8.941176470588236e-05,
    "epoch": 1.470847084708471,
    "step": 335
  },
  {
    "loss": 2.795,
    "grad_norm": 0.9583252668380737,
    "learning_rate": 8.907563025210084e-05,
    "epoch": 1.4752475247524752,
    "step": 336
  },
  {
    "loss": 3.0274,
    "grad_norm": 0.9787155985832214,
    "learning_rate": 8.873949579831933e-05,
    "epoch": 1.4796479647964795,
    "step": 337
  },
  {
    "loss": 2.7072,
    "grad_norm": 0.7296275496482849,
    "learning_rate": 8.840336134453782e-05,
    "epoch": 1.484048404840484,
    "step": 338
  },
  {
    "loss": 2.9003,
    "grad_norm": 0.844656229019165,
    "learning_rate": 8.80672268907563e-05,
    "epoch": 1.4884488448844886,
    "step": 339
  },
  {
    "loss": 2.7078,
    "grad_norm": 0.8764623403549194,
    "learning_rate": 8.77310924369748e-05,
    "epoch": 1.492849284928493,
    "step": 340
  },
  {
    "loss": 2.9894,
    "grad_norm": 0.6835672855377197,
    "learning_rate": 8.739495798319329e-05,
    "epoch": 1.4972497249724972,
    "step": 341
  },
  {
    "loss": 2.5777,
    "grad_norm": 0.8542331457138062,
    "learning_rate": 8.705882352941177e-05,
    "epoch": 1.5016501650165015,
    "step": 342
  },
  {
    "loss": 2.7251,
    "grad_norm": 0.9403756856918335,
    "learning_rate": 8.672268907563026e-05,
    "epoch": 1.506050605060506,
    "step": 343
  },
  {
    "loss": 2.9717,
    "grad_norm": 0.8561168909072876,
    "learning_rate": 8.638655462184874e-05,
    "epoch": 1.5104510451045106,
    "step": 344
  },
  {
    "loss": 2.7376,
    "grad_norm": 0.6990329027175903,
    "learning_rate": 8.605042016806724e-05,
    "epoch": 1.5148514851485149,
    "step": 345
  },
  {
    "loss": 2.748,
    "grad_norm": 0.8437248468399048,
    "learning_rate": 8.571428571428571e-05,
    "epoch": 1.5192519251925192,
    "step": 346
  },
  {
    "loss": 2.7329,
    "grad_norm": 0.792180061340332,
    "learning_rate": 8.53781512605042e-05,
    "epoch": 1.5236523652365237,
    "step": 347
  },
  {
    "loss": 2.904,
    "grad_norm": 0.7899646759033203,
    "learning_rate": 8.50420168067227e-05,
    "epoch": 1.528052805280528,
    "step": 348
  },
  {
    "loss": 3.1287,
    "grad_norm": 0.909322202205658,
    "learning_rate": 8.470588235294118e-05,
    "epoch": 1.5324532453245325,
    "step": 349
  },
  {
    "loss": 2.736,
    "grad_norm": 0.7023309469223022,
    "learning_rate": 8.436974789915967e-05,
    "epoch": 1.5368536853685368,
    "step": 350
  },
  {
    "loss": 2.8419,
    "grad_norm": 0.8392724394798279,
    "learning_rate": 8.403361344537815e-05,
    "epoch": 1.5412541254125411,
    "step": 351
  },
  {
    "loss": 2.8099,
    "grad_norm": 0.8032816648483276,
    "learning_rate": 8.369747899159664e-05,
    "epoch": 1.5456545654565457,
    "step": 352
  },
  {
    "loss": 2.8391,
    "grad_norm": 0.7452762722969055,
    "learning_rate": 8.336134453781514e-05,
    "epoch": 1.5500550055005502,
    "step": 353
  },
  {
    "loss": 2.6838,
    "grad_norm": 0.8295892477035522,
    "learning_rate": 8.302521008403362e-05,
    "epoch": 1.5544554455445545,
    "step": 354
  },
  {
    "loss": 2.9147,
    "grad_norm": 0.7472784519195557,
    "learning_rate": 8.26890756302521e-05,
    "epoch": 1.5588558855885588,
    "step": 355
  },
  {
    "loss": 2.6729,
    "grad_norm": 0.7567137479782104,
    "learning_rate": 8.23529411764706e-05,
    "epoch": 1.5632563256325631,
    "step": 356
  },
  {
    "loss": 2.7295,
    "grad_norm": 0.7468589544296265,
    "learning_rate": 8.201680672268908e-05,
    "epoch": 1.5676567656765676,
    "step": 357
  },
  {
    "loss": 2.8594,
    "grad_norm": 0.7766525149345398,
    "learning_rate": 8.168067226890757e-05,
    "epoch": 1.5720572057205722,
    "step": 358
  },
  {
    "loss": 2.8065,
    "grad_norm": 0.7987014055252075,
    "learning_rate": 8.134453781512605e-05,
    "epoch": 1.5764576457645765,
    "step": 359
  },
  {
    "loss": 2.8456,
    "grad_norm": 0.9113808274269104,
    "learning_rate": 8.100840336134454e-05,
    "epoch": 1.5808580858085808,
    "step": 360
  },
  {
    "loss": 2.7794,
    "grad_norm": 0.8031418323516846,
    "learning_rate": 8.067226890756304e-05,
    "epoch": 1.5852585258525853,
    "step": 361
  },
  {
    "loss": 2.806,
    "grad_norm": 0.7844212651252747,
    "learning_rate": 8.033613445378152e-05,
    "epoch": 1.5896589658965896,
    "step": 362
  },
  {
    "loss": 2.6134,
    "grad_norm": 0.8759738206863403,
    "learning_rate": 8e-05,
    "epoch": 1.5940594059405941,
    "step": 363
  },
  {
    "loss": 2.7466,
    "grad_norm": 0.9551811218261719,
    "learning_rate": 7.966386554621849e-05,
    "epoch": 1.5984598459845984,
    "step": 364
  },
  {
    "loss": 2.6545,
    "grad_norm": 0.8633352518081665,
    "learning_rate": 7.932773109243698e-05,
    "epoch": 1.6028602860286028,
    "step": 365
  },
  {
    "loss": 2.5818,
    "grad_norm": 1.091672658920288,
    "learning_rate": 7.899159663865546e-05,
    "epoch": 1.6072607260726073,
    "step": 366
  },
  {
    "loss": 2.6326,
    "grad_norm": 0.8266956806182861,
    "learning_rate": 7.865546218487395e-05,
    "epoch": 1.6116611661166118,
    "step": 367
  },
  {
    "loss": 2.9615,
    "grad_norm": 0.7935305833816528,
    "learning_rate": 7.831932773109243e-05,
    "epoch": 1.6160616061606161,
    "step": 368
  },
  {
    "loss": 2.7992,
    "grad_norm": 0.7265674471855164,
    "learning_rate": 7.798319327731093e-05,
    "epoch": 1.6204620462046204,
    "step": 369
  },
  {
    "loss": 2.6541,
    "grad_norm": 0.9787818789482117,
    "learning_rate": 7.764705882352942e-05,
    "epoch": 1.6248624862486247,
    "step": 370
  },
  {
    "loss": 2.8113,
    "grad_norm": 0.8073428869247437,
    "learning_rate": 7.73109243697479e-05,
    "epoch": 1.6292629262926293,
    "step": 371
  },
  {
    "loss": 2.625,
    "grad_norm": 0.9284830689430237,
    "learning_rate": 7.697478991596639e-05,
    "epoch": 1.6336633663366338,
    "step": 372
  },
  {
    "loss": 2.7243,
    "grad_norm": 0.8525379300117493,
    "learning_rate": 7.663865546218489e-05,
    "epoch": 1.638063806380638,
    "step": 373
  },
  {
    "loss": 2.6928,
    "grad_norm": 0.7800244688987732,
    "learning_rate": 7.630252100840336e-05,
    "epoch": 1.6424642464246424,
    "step": 374
  },
  {
    "loss": 2.7691,
    "grad_norm": 0.78745436668396,
    "learning_rate": 7.596638655462185e-05,
    "epoch": 1.6468646864686467,
    "step": 375
  },
  {
    "loss": 2.6963,
    "grad_norm": 1.0905072689056396,
    "learning_rate": 7.563025210084033e-05,
    "epoch": 1.6512651265126512,
    "step": 376
  },
  {
    "loss": 2.7563,
    "grad_norm": 0.9512732625007629,
    "learning_rate": 7.529411764705883e-05,
    "epoch": 1.6556655665566558,
    "step": 377
  },
  {
    "loss": 2.6904,
    "grad_norm": 0.8558455109596252,
    "learning_rate": 7.495798319327732e-05,
    "epoch": 1.66006600660066,
    "step": 378
  },
  {
    "loss": 3.0495,
    "grad_norm": 0.790985643863678,
    "learning_rate": 7.46218487394958e-05,
    "epoch": 1.6644664466446644,
    "step": 379
  },
  {
    "loss": 2.9419,
    "grad_norm": 0.7468273043632507,
    "learning_rate": 7.428571428571429e-05,
    "epoch": 1.668866886688669,
    "step": 380
  },
  {
    "loss": 2.5589,
    "grad_norm": 0.9336889982223511,
    "learning_rate": 7.394957983193279e-05,
    "epoch": 1.6732673267326734,
    "step": 381
  },
  {
    "loss": 2.7671,
    "grad_norm": 0.9484606385231018,
    "learning_rate": 7.361344537815127e-05,
    "epoch": 1.6776677667766777,
    "step": 382
  },
  {
    "loss": 2.7815,
    "grad_norm": 0.8135534524917603,
    "learning_rate": 7.327731092436974e-05,
    "epoch": 1.682068206820682,
    "step": 383
  },
  {
    "loss": 2.9951,
    "grad_norm": 0.7277392745018005,
    "learning_rate": 7.294117647058823e-05,
    "epoch": 1.6864686468646863,
    "step": 384
  },
  {
    "loss": 2.7979,
    "grad_norm": 0.7317336201667786,
    "learning_rate": 7.260504201680673e-05,
    "epoch": 1.6908690869086909,
    "step": 385
  },
  {
    "loss": 2.727,
    "grad_norm": 0.7245282530784607,
    "learning_rate": 7.226890756302521e-05,
    "epoch": 1.6952695269526954,
    "step": 386
  },
  {
    "loss": 2.9027,
    "grad_norm": 0.759959876537323,
    "learning_rate": 7.19327731092437e-05,
    "epoch": 1.6996699669966997,
    "step": 387
  },
  {
    "loss": 2.8316,
    "grad_norm": 0.865993320941925,
    "learning_rate": 7.159663865546218e-05,
    "epoch": 1.704070407040704,
    "step": 388
  },
  {
    "loss": 2.8373,
    "grad_norm": 0.8620006442070007,
    "learning_rate": 7.126050420168068e-05,
    "epoch": 1.7084708470847083,
    "step": 389
  },
  {
    "loss": 2.8377,
    "grad_norm": 0.6971022486686707,
    "learning_rate": 7.092436974789917e-05,
    "epoch": 1.7128712871287128,
    "step": 390
  },
  {
    "loss": 2.9103,
    "grad_norm": 0.7130358815193176,
    "learning_rate": 7.058823529411765e-05,
    "epoch": 1.7172717271727174,
    "step": 391
  },
  {
    "loss": 2.6925,
    "grad_norm": 0.7759323716163635,
    "learning_rate": 7.025210084033613e-05,
    "epoch": 1.7216721672167217,
    "step": 392
  },
  {
    "loss": 2.813,
    "grad_norm": 0.9288842082023621,
    "learning_rate": 6.991596638655463e-05,
    "epoch": 1.726072607260726,
    "step": 393
  },
  {
    "loss": 2.8154,
    "grad_norm": 0.83884197473526,
    "learning_rate": 6.957983193277311e-05,
    "epoch": 1.7304730473047305,
    "step": 394
  },
  {
    "loss": 2.9611,
    "grad_norm": 0.6729629635810852,
    "learning_rate": 6.92436974789916e-05,
    "epoch": 1.734873487348735,
    "step": 395
  },
  {
    "loss": 2.8816,
    "grad_norm": 0.8025194406509399,
    "learning_rate": 6.890756302521008e-05,
    "epoch": 1.7392739273927393,
    "step": 396
  },
  {
    "loss": 2.72,
    "grad_norm": 0.9628185629844666,
    "learning_rate": 6.857142857142858e-05,
    "epoch": 1.7436743674367436,
    "step": 397
  },
  {
    "loss": 2.8516,
    "grad_norm": 0.7356510758399963,
    "learning_rate": 6.823529411764707e-05,
    "epoch": 1.748074807480748,
    "step": 398
  },
  {
    "loss": 2.9081,
    "grad_norm": 0.8327540159225464,
    "learning_rate": 6.789915966386555e-05,
    "epoch": 1.7524752475247525,
    "step": 399
  },
  {
    "loss": 2.8056,
    "grad_norm": 0.8489276766777039,
    "learning_rate": 6.756302521008404e-05,
    "epoch": 1.756875687568757,
    "step": 400
  },
  {
    "loss": 2.5113,
    "grad_norm": 0.8683672547340393,
    "learning_rate": 6.722689075630254e-05,
    "epoch": 1.7612761276127613,
    "step": 401
  },
  {
    "loss": 2.6789,
    "grad_norm": 0.804157018661499,
    "learning_rate": 6.689075630252101e-05,
    "epoch": 1.7656765676567656,
    "step": 402
  },
  {
    "loss": 2.8289,
    "grad_norm": 0.6615346074104309,
    "learning_rate": 6.65546218487395e-05,
    "epoch": 1.77007700770077,
    "step": 403
  },
  {
    "loss": 2.5705,
    "grad_norm": 0.8978198766708374,
    "learning_rate": 6.621848739495798e-05,
    "epoch": 1.7744774477447744,
    "step": 404
  },
  {
    "loss": 2.8077,
    "grad_norm": 0.7342696189880371,
    "learning_rate": 6.588235294117648e-05,
    "epoch": 1.778877887788779,
    "step": 405
  },
  {
    "loss": 2.696,
    "grad_norm": 0.8770032525062561,
    "learning_rate": 6.554621848739496e-05,
    "epoch": 1.7832783278327833,
    "step": 406
  },
  {
    "loss": 2.8305,
    "grad_norm": 0.8373759984970093,
    "learning_rate": 6.521008403361345e-05,
    "epoch": 1.7876787678767876,
    "step": 407
  },
  {
    "loss": 2.7441,
    "grad_norm": 0.9051594734191895,
    "learning_rate": 6.487394957983193e-05,
    "epoch": 1.7920792079207921,
    "step": 408
  },
  {
    "loss": 2.6924,
    "grad_norm": 0.8850083947181702,
    "learning_rate": 6.453781512605043e-05,
    "epoch": 1.7964796479647966,
    "step": 409
  },
  {
    "loss": 2.7234,
    "grad_norm": 0.927768349647522,
    "learning_rate": 6.420168067226892e-05,
    "epoch": 1.800880088008801,
    "step": 410
  },
  {
    "loss": 2.804,
    "grad_norm": 0.8114985227584839,
    "learning_rate": 6.386554621848739e-05,
    "epoch": 1.8052805280528053,
    "step": 411
  },
  {
    "loss": 2.9535,
    "grad_norm": 0.8277252316474915,
    "learning_rate": 6.352941176470588e-05,
    "epoch": 1.8096809680968096,
    "step": 412
  },
  {
    "loss": 2.7025,
    "grad_norm": 1.0082964897155762,
    "learning_rate": 6.319327731092438e-05,
    "epoch": 1.814081408140814,
    "step": 413
  },
  {
    "loss": 2.6595,
    "grad_norm": 0.7468433976173401,
    "learning_rate": 6.285714285714286e-05,
    "epoch": 1.8184818481848186,
    "step": 414
  },
  {
    "loss": 2.9405,
    "grad_norm": 0.712445080280304,
    "learning_rate": 6.252100840336135e-05,
    "epoch": 1.822882288228823,
    "step": 415
  },
  {
    "loss": 2.8041,
    "grad_norm": 0.7938172221183777,
    "learning_rate": 6.218487394957983e-05,
    "epoch": 1.8272827282728272,
    "step": 416
  },
  {
    "loss": 2.488,
    "grad_norm": 0.7483130693435669,
    "learning_rate": 6.184873949579833e-05,
    "epoch": 1.8316831683168315,
    "step": 417
  },
  {
    "loss": 2.9003,
    "grad_norm": 0.919029712677002,
    "learning_rate": 6.151260504201682e-05,
    "epoch": 1.836083608360836,
    "step": 418
  },
  {
    "loss": 2.7914,
    "grad_norm": 0.7144599556922913,
    "learning_rate": 6.11764705882353e-05,
    "epoch": 1.8404840484048406,
    "step": 419
  },
  {
    "loss": 2.6156,
    "grad_norm": 0.8443020582199097,
    "learning_rate": 6.084033613445378e-05,
    "epoch": 1.844884488448845,
    "step": 420
  },
  {
    "loss": 3.0104,
    "grad_norm": 0.861000657081604,
    "learning_rate": 6.0504201680672267e-05,
    "epoch": 1.8492849284928492,
    "step": 421
  },
  {
    "loss": 2.5833,
    "grad_norm": 1.1675004959106445,
    "learning_rate": 6.016806722689076e-05,
    "epoch": 1.8536853685368537,
    "step": 422
  },
  {
    "loss": 2.7604,
    "grad_norm": 0.8199593424797058,
    "learning_rate": 5.9831932773109244e-05,
    "epoch": 1.858085808580858,
    "step": 423
  },
  {
    "loss": 2.6895,
    "grad_norm": 0.7240762114524841,
    "learning_rate": 5.9495798319327737e-05,
    "epoch": 1.8624862486248626,
    "step": 424
  },
  {
    "loss": 2.8492,
    "grad_norm": 0.8031548261642456,
    "learning_rate": 5.915966386554622e-05,
    "epoch": 1.8668866886688669,
    "step": 425
  },
  {
    "loss": 2.6905,
    "grad_norm": 0.8072637319564819,
    "learning_rate": 5.882352941176471e-05,
    "epoch": 1.8712871287128712,
    "step": 426
  },
  {
    "loss": 2.6405,
    "grad_norm": 0.7570180892944336,
    "learning_rate": 5.84873949579832e-05,
    "epoch": 1.8756875687568757,
    "step": 427
  },
  {
    "loss": 2.5832,
    "grad_norm": 0.9133287668228149,
    "learning_rate": 5.8151260504201685e-05,
    "epoch": 1.8800880088008802,
    "step": 428
  },
  {
    "loss": 2.9996,
    "grad_norm": 0.7901890277862549,
    "learning_rate": 5.781512605042018e-05,
    "epoch": 1.8844884488448845,
    "step": 429
  },
  {
    "loss": 2.8146,
    "grad_norm": 0.8248451948165894,
    "learning_rate": 5.7478991596638656e-05,
    "epoch": 1.8888888888888888,
    "step": 430
  },
  {
    "loss": 2.7078,
    "grad_norm": 0.7504086494445801,
    "learning_rate": 5.714285714285714e-05,
    "epoch": 1.8932893289328931,
    "step": 431
  },
  {
    "loss": 2.7449,
    "grad_norm": 0.6852871775627136,
    "learning_rate": 5.6806722689075634e-05,
    "epoch": 1.8976897689768977,
    "step": 432
  },
  {
    "loss": 2.8651,
    "grad_norm": 0.7311152219772339,
    "learning_rate": 5.647058823529412e-05,
    "epoch": 1.9020902090209022,
    "step": 433
  },
  {
    "loss": 2.7719,
    "grad_norm": 0.8527572154998779,
    "learning_rate": 5.6134453781512605e-05,
    "epoch": 1.9064906490649065,
    "step": 434
  },
  {
    "loss": 2.8838,
    "grad_norm": 0.816210925579071,
    "learning_rate": 5.57983193277311e-05,
    "epoch": 1.9108910891089108,
    "step": 435
  },
  {
    "loss": 2.7644,
    "grad_norm": 0.8986364006996155,
    "learning_rate": 5.546218487394958e-05,
    "epoch": 1.9152915291529153,
    "step": 436
  },
  {
    "loss": 2.7363,
    "grad_norm": 0.9465794563293457,
    "learning_rate": 5.5126050420168075e-05,
    "epoch": 1.9196919691969196,
    "step": 437
  },
  {
    "loss": 2.4571,
    "grad_norm": 0.7629629373550415,
    "learning_rate": 5.478991596638656e-05,
    "epoch": 1.9240924092409242,
    "step": 438
  },
  {
    "loss": 2.4449,
    "grad_norm": 0.9661117792129517,
    "learning_rate": 5.445378151260504e-05,
    "epoch": 1.9284928492849285,
    "step": 439
  },
  {
    "loss": 2.7303,
    "grad_norm": 0.7260268330574036,
    "learning_rate": 5.411764705882353e-05,
    "epoch": 1.9328932893289328,
    "step": 440
  },
  {
    "loss": 2.6528,
    "grad_norm": 0.8035115003585815,
    "learning_rate": 5.378151260504202e-05,
    "epoch": 1.9372937293729373,
    "step": 441
  },
  {
    "loss": 2.3251,
    "grad_norm": 0.7750284075737,
    "learning_rate": 5.34453781512605e-05,
    "epoch": 1.9416941694169418,
    "step": 442
  },
  {
    "loss": 2.7616,
    "grad_norm": 0.8822467923164368,
    "learning_rate": 5.3109243697478995e-05,
    "epoch": 1.9460946094609461,
    "step": 443
  },
  {
    "loss": 3.0616,
    "grad_norm": 0.7976251244544983,
    "learning_rate": 5.277310924369748e-05,
    "epoch": 1.9504950495049505,
    "step": 444
  },
  {
    "loss": 2.8934,
    "grad_norm": 0.9234274625778198,
    "learning_rate": 5.243697478991597e-05,
    "epoch": 1.9548954895489548,
    "step": 445
  },
  {
    "loss": 2.8823,
    "grad_norm": 0.7331113815307617,
    "learning_rate": 5.210084033613446e-05,
    "epoch": 1.9592959295929593,
    "step": 446
  },
  {
    "loss": 2.8733,
    "grad_norm": 0.8743939995765686,
    "learning_rate": 5.176470588235295e-05,
    "epoch": 1.9636963696369638,
    "step": 447
  },
  {
    "loss": 2.8069,
    "grad_norm": 0.7880393266677856,
    "learning_rate": 5.142857142857143e-05,
    "epoch": 1.9680968096809681,
    "step": 448
  },
  {
    "loss": 2.8113,
    "grad_norm": 0.8667842745780945,
    "learning_rate": 5.1092436974789914e-05,
    "epoch": 1.9724972497249724,
    "step": 449
  },
  {
    "loss": 2.7157,
    "grad_norm": 0.9382930397987366,
    "learning_rate": 5.07563025210084e-05,
    "epoch": 1.976897689768977,
    "step": 450
  },
  {
    "loss": 2.8767,
    "grad_norm": 0.9209972620010376,
    "learning_rate": 5.042016806722689e-05,
    "epoch": 1.9812981298129813,
    "step": 451
  },
  {
    "loss": 2.6605,
    "grad_norm": 0.7867906093597412,
    "learning_rate": 5.008403361344538e-05,
    "epoch": 1.9856985698569858,
    "step": 452
  },
  {
    "loss": 2.7821,
    "grad_norm": 0.7051613926887512,
    "learning_rate": 4.974789915966387e-05,
    "epoch": 1.99009900990099,
    "step": 453
  },
  {
    "loss": 2.8147,
    "grad_norm": 0.8814588189125061,
    "learning_rate": 4.9411764705882355e-05,
    "epoch": 1.9944994499449944,
    "step": 454
  },
  {
    "loss": 2.8511,
    "grad_norm": 0.797398030757904,
    "learning_rate": 4.907563025210084e-05,
    "epoch": 1.998899889988999,
    "step": 455
  },
  {
    "loss": 2.9067,
    "grad_norm": 2.2706215381622314,
    "learning_rate": 4.8739495798319326e-05,
    "epoch": 2.0,
    "step": 456
  },
  {
    "loss": 2.7152,
    "grad_norm": 0.8236144781112671,
    "learning_rate": 4.840336134453782e-05,
    "epoch": 2.0044004400440043,
    "step": 457
  },
  {
    "loss": 2.5194,
    "grad_norm": 0.7731046080589294,
    "learning_rate": 4.8067226890756304e-05,
    "epoch": 2.0088008800880086,
    "step": 458
  },
  {
    "loss": 2.9271,
    "grad_norm": 1.0610498189926147,
    "learning_rate": 4.7731092436974796e-05,
    "epoch": 2.0132013201320134,
    "step": 459
  },
  {
    "loss": 2.5119,
    "grad_norm": 0.9861636757850647,
    "learning_rate": 4.7394957983193275e-05,
    "epoch": 2.0176017601760177,
    "step": 460
  },
  {
    "loss": 2.5984,
    "grad_norm": 0.8323330879211426,
    "learning_rate": 4.705882352941177e-05,
    "epoch": 2.022002200220022,
    "step": 461
  },
  {
    "loss": 2.6407,
    "grad_norm": 0.9026651978492737,
    "learning_rate": 4.672268907563025e-05,
    "epoch": 2.0264026402640263,
    "step": 462
  },
  {
    "loss": 2.6396,
    "grad_norm": 0.8357550501823425,
    "learning_rate": 4.6386554621848745e-05,
    "epoch": 2.0308030803080306,
    "step": 463
  },
  {
    "loss": 2.7161,
    "grad_norm": 0.887241005897522,
    "learning_rate": 4.6050420168067224e-05,
    "epoch": 2.0352035203520353,
    "step": 464
  },
  {
    "loss": 2.6143,
    "grad_norm": 0.8206489682197571,
    "learning_rate": 4.5714285714285716e-05,
    "epoch": 2.0396039603960396,
    "step": 465
  },
  {
    "loss": 2.7281,
    "grad_norm": 0.8427479267120361,
    "learning_rate": 4.53781512605042e-05,
    "epoch": 2.044004400440044,
    "step": 466
  },
  {
    "loss": 2.735,
    "grad_norm": 0.7320525646209717,
    "learning_rate": 4.5042016806722694e-05,
    "epoch": 2.0484048404840483,
    "step": 467
  },
  {
    "loss": 2.9203,
    "grad_norm": 0.8913852572441101,
    "learning_rate": 4.470588235294118e-05,
    "epoch": 2.052805280528053,
    "step": 468
  },
  {
    "loss": 2.7568,
    "grad_norm": 0.6573203206062317,
    "learning_rate": 4.4369747899159665e-05,
    "epoch": 2.0572057205720573,
    "step": 469
  },
  {
    "loss": 2.5906,
    "grad_norm": 0.8845142126083374,
    "learning_rate": 4.403361344537815e-05,
    "epoch": 2.0616061606160616,
    "step": 470
  },
  {
    "loss": 2.6023,
    "grad_norm": 0.8395589590072632,
    "learning_rate": 4.369747899159664e-05,
    "epoch": 2.066006600660066,
    "step": 471
  },
  {
    "loss": 2.6477,
    "grad_norm": 0.7325526475906372,
    "learning_rate": 4.336134453781513e-05,
    "epoch": 2.0704070407040702,
    "step": 472
  },
  {
    "loss": 2.7614,
    "grad_norm": 0.8674635291099548,
    "learning_rate": 4.302521008403362e-05,
    "epoch": 2.074807480748075,
    "step": 473
  },
  {
    "loss": 2.628,
    "grad_norm": 0.9928527474403381,
    "learning_rate": 4.26890756302521e-05,
    "epoch": 2.0792079207920793,
    "step": 474
  },
  {
    "loss": 2.6974,
    "grad_norm": 0.7609483599662781,
    "learning_rate": 4.235294117647059e-05,
    "epoch": 2.0836083608360836,
    "step": 475
  },
  {
    "loss": 2.688,
    "grad_norm": 0.814937949180603,
    "learning_rate": 4.201680672268908e-05,
    "epoch": 2.088008800880088,
    "step": 476
  },
  {
    "loss": 2.9036,
    "grad_norm": 0.8108230829238892,
    "learning_rate": 4.168067226890757e-05,
    "epoch": 2.092409240924092,
    "step": 477
  },
  {
    "loss": 2.6951,
    "grad_norm": 0.8507907390594482,
    "learning_rate": 4.134453781512605e-05,
    "epoch": 2.096809680968097,
    "step": 478
  },
  {
    "loss": 2.8093,
    "grad_norm": 0.8659224510192871,
    "learning_rate": 4.100840336134454e-05,
    "epoch": 2.1012101210121013,
    "step": 479
  },
  {
    "loss": 2.5075,
    "grad_norm": 1.022226333618164,
    "learning_rate": 4.0672268907563026e-05,
    "epoch": 2.1056105610561056,
    "step": 480
  },
  {
    "loss": 2.7472,
    "grad_norm": 0.9835419654846191,
    "learning_rate": 4.033613445378152e-05,
    "epoch": 2.11001100110011,
    "step": 481
  },
  {
    "loss": 2.6223,
    "grad_norm": 0.8444758057594299,
    "learning_rate": 4e-05,
    "epoch": 2.1144114411441146,
    "step": 482
  },
  {
    "loss": 2.6038,
    "grad_norm": 0.9343941807746887,
    "learning_rate": 3.966386554621849e-05,
    "epoch": 2.118811881188119,
    "step": 483
  },
  {
    "loss": 2.6969,
    "grad_norm": 0.7982450127601624,
    "learning_rate": 3.9327731092436974e-05,
    "epoch": 2.1232123212321232,
    "step": 484
  },
  {
    "loss": 2.6459,
    "grad_norm": 0.7500283718109131,
    "learning_rate": 3.8991596638655467e-05,
    "epoch": 2.1276127612761275,
    "step": 485
  },
  {
    "loss": 2.5818,
    "grad_norm": 0.8416764736175537,
    "learning_rate": 3.865546218487395e-05,
    "epoch": 2.132013201320132,
    "step": 486
  },
  {
    "loss": 2.3923,
    "grad_norm": 0.9398849606513977,
    "learning_rate": 3.8319327731092444e-05,
    "epoch": 2.1364136413641366,
    "step": 487
  },
  {
    "loss": 2.6241,
    "grad_norm": 0.843430757522583,
    "learning_rate": 3.798319327731092e-05,
    "epoch": 2.140814081408141,
    "step": 488
  },
  {
    "loss": 2.4041,
    "grad_norm": 0.8545964956283569,
    "learning_rate": 3.7647058823529415e-05,
    "epoch": 2.145214521452145,
    "step": 489
  },
  {
    "loss": 2.8421,
    "grad_norm": 0.9033119082450867,
    "learning_rate": 3.73109243697479e-05,
    "epoch": 2.1496149614961495,
    "step": 490
  },
  {
    "loss": 2.7052,
    "grad_norm": 1.095144510269165,
    "learning_rate": 3.697478991596639e-05,
    "epoch": 2.1540154015401543,
    "step": 491
  },
  {
    "loss": 2.6114,
    "grad_norm": 1.01564621925354,
    "learning_rate": 3.663865546218487e-05,
    "epoch": 2.1584158415841586,
    "step": 492
  },
  {
    "loss": 2.7168,
    "grad_norm": 0.9058207869529724,
    "learning_rate": 3.6302521008403364e-05,
    "epoch": 2.162816281628163,
    "step": 493
  },
  {
    "loss": 2.7629,
    "grad_norm": 0.927505612373352,
    "learning_rate": 3.596638655462185e-05,
    "epoch": 2.167216721672167,
    "step": 494
  },
  {
    "loss": 2.7975,
    "grad_norm": 0.8181199431419373,
    "learning_rate": 3.563025210084034e-05,
    "epoch": 2.1716171617161715,
    "step": 495
  },
  {
    "loss": 2.5524,
    "grad_norm": 0.962418258190155,
    "learning_rate": 3.529411764705883e-05,
    "epoch": 2.1760176017601762,
    "step": 496
  },
  {
    "loss": 2.8144,
    "grad_norm": 0.7965072989463806,
    "learning_rate": 3.495798319327731e-05,
    "epoch": 2.1804180418041805,
    "step": 497
  },
  {
    "loss": 2.7043,
    "grad_norm": 0.8948770761489868,
    "learning_rate": 3.46218487394958e-05,
    "epoch": 2.184818481848185,
    "step": 498
  },
  {
    "loss": 2.492,
    "grad_norm": 0.728932797908783,
    "learning_rate": 3.428571428571429e-05,
    "epoch": 2.189218921892189,
    "step": 499
  },
  {
    "loss": 2.4726,
    "grad_norm": 0.9403459429740906,
    "learning_rate": 3.3949579831932776e-05,
    "epoch": 2.1936193619361934,
    "step": 500
  },
  {
    "loss": 2.8014,
    "grad_norm": 0.7935392260551453,
    "learning_rate": 3.361344537815127e-05,
    "epoch": 2.198019801980198,
    "step": 501
  },
  {
    "loss": 2.8286,
    "grad_norm": 1.0302040576934814,
    "learning_rate": 3.327731092436975e-05,
    "epoch": 2.2024202420242025,
    "step": 502
  },
  {
    "loss": 2.5201,
    "grad_norm": 0.7890738844871521,
    "learning_rate": 3.294117647058824e-05,
    "epoch": 2.206820682068207,
    "step": 503
  },
  {
    "loss": 2.9579,
    "grad_norm": 0.8859487771987915,
    "learning_rate": 3.2605042016806725e-05,
    "epoch": 2.211221122112211,
    "step": 504
  },
  {
    "loss": 2.6878,
    "grad_norm": 0.7235979437828064,
    "learning_rate": 3.226890756302522e-05,
    "epoch": 2.2156215621562154,
    "step": 505
  },
  {
    "loss": 2.905,
    "grad_norm": 0.816119909286499,
    "learning_rate": 3.1932773109243696e-05,
    "epoch": 2.22002200220022,
    "step": 506
  },
  {
    "loss": 2.7081,
    "grad_norm": 0.8729830980300903,
    "learning_rate": 3.159663865546219e-05,
    "epoch": 2.2244224422442245,
    "step": 507
  },
  {
    "loss": 2.9127,
    "grad_norm": 0.7452929019927979,
    "learning_rate": 3.1260504201680673e-05,
    "epoch": 2.228822882288229,
    "step": 508
  },
  {
    "loss": 2.6441,
    "grad_norm": 0.9932128190994263,
    "learning_rate": 3.0924369747899166e-05,
    "epoch": 2.233223322332233,
    "step": 509
  },
  {
    "loss": 2.6028,
    "grad_norm": 0.9656256437301636,
    "learning_rate": 3.058823529411765e-05,
    "epoch": 2.237623762376238,
    "step": 510
  },
  {
    "loss": 2.6999,
    "grad_norm": 0.9259200096130371,
    "learning_rate": 3.0252100840336133e-05,
    "epoch": 2.242024202420242,
    "step": 511
  },
  {
    "loss": 2.762,
    "grad_norm": 0.8690330982208252,
    "learning_rate": 2.9915966386554622e-05,
    "epoch": 2.2464246424642464,
    "step": 512
  },
  {
    "loss": 2.5737,
    "grad_norm": 0.9932162165641785,
    "learning_rate": 2.957983193277311e-05,
    "epoch": 2.2508250825082508,
    "step": 513
  },
  {
    "loss": 2.9506,
    "grad_norm": 0.9541937112808228,
    "learning_rate": 2.92436974789916e-05,
    "epoch": 2.255225522552255,
    "step": 514
  },
  {
    "loss": 2.8328,
    "grad_norm": 0.7609009146690369,
    "learning_rate": 2.890756302521009e-05,
    "epoch": 2.25962596259626,
    "step": 515
  },
  {
    "loss": 2.6101,
    "grad_norm": 0.8220157027244568,
    "learning_rate": 2.857142857142857e-05,
    "epoch": 2.264026402640264,
    "step": 516
  },
  {
    "loss": 2.5953,
    "grad_norm": 0.8067742586135864,
    "learning_rate": 2.823529411764706e-05,
    "epoch": 2.2684268426842684,
    "step": 517
  },
  {
    "loss": 2.5394,
    "grad_norm": 1.143095850944519,
    "learning_rate": 2.789915966386555e-05,
    "epoch": 2.2728272827282727,
    "step": 518
  },
  {
    "loss": 2.6847,
    "grad_norm": 0.8367123603820801,
    "learning_rate": 2.7563025210084037e-05,
    "epoch": 2.2772277227722775,
    "step": 519
  },
  {
    "loss": 2.6552,
    "grad_norm": 0.9776541590690613,
    "learning_rate": 2.722689075630252e-05,
    "epoch": 2.281628162816282,
    "step": 520
  },
  {
    "loss": 2.7519,
    "grad_norm": 0.7524437308311462,
    "learning_rate": 2.689075630252101e-05,
    "epoch": 2.286028602860286,
    "step": 521
  },
  {
    "loss": 2.6851,
    "grad_norm": 0.8491640090942383,
    "learning_rate": 2.6554621848739497e-05,
    "epoch": 2.2904290429042904,
    "step": 522
  },
  {
    "loss": 2.8267,
    "grad_norm": 0.778921902179718,
    "learning_rate": 2.6218487394957986e-05,
    "epoch": 2.2948294829482947,
    "step": 523
  },
  {
    "loss": 2.7869,
    "grad_norm": 0.8055436015129089,
    "learning_rate": 2.5882352941176475e-05,
    "epoch": 2.299229922992299,
    "step": 524
  },
  {
    "loss": 2.5747,
    "grad_norm": 1.0314974784851074,
    "learning_rate": 2.5546218487394957e-05,
    "epoch": 2.3036303630363038,
    "step": 525
  },
  {
    "loss": 2.8282,
    "grad_norm": 0.8036290407180786,
    "learning_rate": 2.5210084033613446e-05,
    "epoch": 2.308030803080308,
    "step": 526
  },
  {
    "loss": 2.8792,
    "grad_norm": 0.8956916332244873,
    "learning_rate": 2.4873949579831935e-05,
    "epoch": 2.3124312431243124,
    "step": 527
  },
  {
    "loss": 2.6106,
    "grad_norm": 1.106376051902771,
    "learning_rate": 2.453781512605042e-05,
    "epoch": 2.3168316831683167,
    "step": 528
  },
  {
    "loss": 2.7012,
    "grad_norm": 0.913290798664093,
    "learning_rate": 2.420168067226891e-05,
    "epoch": 2.3212321232123214,
    "step": 529
  },
  {
    "loss": 2.6771,
    "grad_norm": 0.7016757130622864,
    "learning_rate": 2.3865546218487398e-05,
    "epoch": 2.3256325632563257,
    "step": 530
  },
  {
    "loss": 2.5886,
    "grad_norm": 1.3385337591171265,
    "learning_rate": 2.3529411764705884e-05,
    "epoch": 2.33003300330033,
    "step": 531
  },
  {
    "loss": 2.6253,
    "grad_norm": 0.7739492058753967,
    "learning_rate": 2.3193277310924373e-05,
    "epoch": 2.3344334433443343,
    "step": 532
  },
  {
    "loss": 2.4989,
    "grad_norm": 1.0662845373153687,
    "learning_rate": 2.2857142857142858e-05,
    "epoch": 2.3388338833883386,
    "step": 533
  },
  {
    "loss": 2.5188,
    "grad_norm": 0.9285594820976257,
    "learning_rate": 2.2521008403361347e-05,
    "epoch": 2.3432343234323434,
    "step": 534
  },
  {
    "loss": 2.6231,
    "grad_norm": 0.9248674511909485,
    "learning_rate": 2.2184873949579832e-05,
    "epoch": 2.3476347634763477,
    "step": 535
  },
  {
    "loss": 2.6971,
    "grad_norm": 0.882972776889801,
    "learning_rate": 2.184873949579832e-05,
    "epoch": 2.352035203520352,
    "step": 536
  },
  {
    "loss": 2.5411,
    "grad_norm": 0.8821325302124023,
    "learning_rate": 2.151260504201681e-05,
    "epoch": 2.3564356435643563,
    "step": 537
  },
  {
    "loss": 2.7644,
    "grad_norm": 0.8939436078071594,
    "learning_rate": 2.1176470588235296e-05,
    "epoch": 2.360836083608361,
    "step": 538
  },
  {
    "loss": 2.6329,
    "grad_norm": 0.8540006875991821,
    "learning_rate": 2.0840336134453785e-05,
    "epoch": 2.3652365236523654,
    "step": 539
  },
  {
    "loss": 2.6147,
    "grad_norm": 0.8298600912094116,
    "learning_rate": 2.050420168067227e-05,
    "epoch": 2.3696369636963697,
    "step": 540
  },
  {
    "loss": 2.7519,
    "grad_norm": 0.9191837310791016,
    "learning_rate": 2.016806722689076e-05,
    "epoch": 2.374037403740374,
    "step": 541
  },
  {
    "loss": 2.7767,
    "grad_norm": 0.8323971033096313,
    "learning_rate": 1.9831932773109244e-05,
    "epoch": 2.3784378437843783,
    "step": 542
  },
  {
    "loss": 2.6011,
    "grad_norm": 0.9705237150192261,
    "learning_rate": 1.9495798319327733e-05,
    "epoch": 2.382838283828383,
    "step": 543
  },
  {
    "loss": 2.7917,
    "grad_norm": 0.8904041647911072,
    "learning_rate": 1.9159663865546222e-05,
    "epoch": 2.3872387238723873,
    "step": 544
  },
  {
    "loss": 2.5662,
    "grad_norm": 1.0500329732894897,
    "learning_rate": 1.8823529411764708e-05,
    "epoch": 2.3916391639163916,
    "step": 545
  },
  {
    "loss": 2.6233,
    "grad_norm": 1.1387144327163696,
    "learning_rate": 1.8487394957983196e-05,
    "epoch": 2.396039603960396,
    "step": 546
  },
  {
    "loss": 2.7266,
    "grad_norm": 0.9686482548713684,
    "learning_rate": 1.8151260504201682e-05,
    "epoch": 2.4004400440044003,
    "step": 547
  },
  {
    "loss": 2.8545,
    "grad_norm": 1.0116461515426636,
    "learning_rate": 1.781512605042017e-05,
    "epoch": 2.404840484048405,
    "step": 548
  },
  {
    "loss": 2.5734,
    "grad_norm": 1.0405981540679932,
    "learning_rate": 1.7478991596638656e-05,
    "epoch": 2.4092409240924093,
    "step": 549
  },
  {
    "loss": 2.7049,
    "grad_norm": 0.7591031193733215,
    "learning_rate": 1.7142857142857145e-05,
    "epoch": 2.4136413641364136,
    "step": 550
  },
  {
    "loss": 2.9338,
    "grad_norm": 0.9806702136993408,
    "learning_rate": 1.6806722689075634e-05,
    "epoch": 2.418041804180418,
    "step": 551
  },
  {
    "loss": 2.6725,
    "grad_norm": 1.256486415863037,
    "learning_rate": 1.647058823529412e-05,
    "epoch": 2.4224422442244222,
    "step": 552
  },
  {
    "loss": 2.6051,
    "grad_norm": 0.8130475282669067,
    "learning_rate": 1.613445378151261e-05,
    "epoch": 2.426842684268427,
    "step": 553
  },
  {
    "loss": 2.6989,
    "grad_norm": 0.8171542882919312,
    "learning_rate": 1.5798319327731094e-05,
    "epoch": 2.4312431243124313,
    "step": 554
  },
  {
    "loss": 2.8366,
    "grad_norm": 0.8701655268669128,
    "learning_rate": 1.5462184873949583e-05,
    "epoch": 2.4356435643564356,
    "step": 555
  },
  {
    "loss": 2.6439,
    "grad_norm": 1.0385397672653198,
    "learning_rate": 1.5126050420168067e-05,
    "epoch": 2.44004400440044,
    "step": 556
  },
  {
    "loss": 2.6405,
    "grad_norm": 0.7820117473602295,
    "learning_rate": 1.4789915966386556e-05,
    "epoch": 2.4444444444444446,
    "step": 557
  },
  {
    "loss": 2.6553,
    "grad_norm": 0.8111140727996826,
    "learning_rate": 1.4453781512605044e-05,
    "epoch": 2.448844884488449,
    "step": 558
  },
  {
    "loss": 2.8231,
    "grad_norm": 0.9572077393531799,
    "learning_rate": 1.411764705882353e-05,
    "epoch": 2.4532453245324533,
    "step": 559
  },
  {
    "loss": 2.731,
    "grad_norm": 0.8152912855148315,
    "learning_rate": 1.3781512605042019e-05,
    "epoch": 2.4576457645764576,
    "step": 560
  },
  {
    "loss": 2.687,
    "grad_norm": 0.9923820495605469,
    "learning_rate": 1.3445378151260504e-05,
    "epoch": 2.462046204620462,
    "step": 561
  },
  {
    "loss": 2.6061,
    "grad_norm": 1.0915032625198364,
    "learning_rate": 1.3109243697478993e-05,
    "epoch": 2.4664466446644666,
    "step": 562
  },
  {
    "loss": 2.5197,
    "grad_norm": 0.8870192170143127,
    "learning_rate": 1.2773109243697479e-05,
    "epoch": 2.470847084708471,
    "step": 563
  },
  {
    "loss": 2.6513,
    "grad_norm": 0.8160111308097839,
    "learning_rate": 1.2436974789915967e-05,
    "epoch": 2.4752475247524752,
    "step": 564
  },
  {
    "loss": 2.6783,
    "grad_norm": 0.8461321592330933,
    "learning_rate": 1.2100840336134455e-05,
    "epoch": 2.4796479647964795,
    "step": 565
  },
  {
    "loss": 2.7047,
    "grad_norm": 0.8049362897872925,
    "learning_rate": 1.1764705882352942e-05,
    "epoch": 2.4840484048404843,
    "step": 566
  },
  {
    "loss": 2.7338,
    "grad_norm": 0.9869232773780823,
    "learning_rate": 1.1428571428571429e-05,
    "epoch": 2.4884488448844886,
    "step": 567
  },
  {
    "loss": 2.632,
    "grad_norm": 0.8510556817054749,
    "learning_rate": 1.1092436974789916e-05,
    "epoch": 2.492849284928493,
    "step": 568
  },
  {
    "loss": 2.3878,
    "grad_norm": 0.894485592842102,
    "learning_rate": 1.0756302521008405e-05,
    "epoch": 2.497249724972497,
    "step": 569
  },
  {
    "loss": 2.5899,
    "grad_norm": 0.8124459385871887,
    "learning_rate": 1.0420168067226892e-05,
    "epoch": 2.5016501650165015,
    "step": 570
  },
  {
    "loss": 2.7072,
    "grad_norm": 0.7802164554595947,
    "learning_rate": 1.008403361344538e-05,
    "epoch": 2.506050605060506,
    "step": 571
  },
  {
    "loss": 2.6101,
    "grad_norm": 0.967399537563324,
    "learning_rate": 9.747899159663867e-06,
    "epoch": 2.5104510451045106,
    "step": 572
  },
  {
    "loss": 2.8017,
    "grad_norm": 1.059965968132019,
    "learning_rate": 9.411764705882354e-06,
    "epoch": 2.514851485148515,
    "step": 573
  },
  {
    "loss": 2.6283,
    "grad_norm": 0.8575851917266846,
    "learning_rate": 9.075630252100841e-06,
    "epoch": 2.519251925192519,
    "step": 574
  },
  {
    "loss": 2.4972,
    "grad_norm": 0.8159731030464172,
    "learning_rate": 8.739495798319328e-06,
    "epoch": 2.523652365236524,
    "step": 575
  },
  {
    "loss": 2.7099,
    "grad_norm": 1.0020495653152466,
    "learning_rate": 8.403361344537817e-06,
    "epoch": 2.5280528052805282,
    "step": 576
  },
  {
    "loss": 2.5804,
    "grad_norm": 0.8823985457420349,
    "learning_rate": 8.067226890756304e-06,
    "epoch": 2.5324532453245325,
    "step": 577
  },
  {
    "loss": 2.6518,
    "grad_norm": 1.0216847658157349,
    "learning_rate": 7.731092436974791e-06,
    "epoch": 2.536853685368537,
    "step": 578
  },
  {
    "loss": 2.696,
    "grad_norm": 0.8915552496910095,
    "learning_rate": 7.394957983193278e-06,
    "epoch": 2.541254125412541,
    "step": 579
  },
  {
    "loss": 2.6099,
    "grad_norm": 1.1793804168701172,
    "learning_rate": 7.058823529411765e-06,
    "epoch": 2.5456545654565454,
    "step": 580
  },
  {
    "loss": 2.6372,
    "grad_norm": 1.0216476917266846,
    "learning_rate": 6.722689075630252e-06,
    "epoch": 2.55005500550055,
    "step": 581
  },
  {
    "loss": 2.7704,
    "grad_norm": 0.9771246910095215,
    "learning_rate": 6.386554621848739e-06,
    "epoch": 2.5544554455445545,
    "step": 582
  },
  {
    "loss": 2.8014,
    "grad_norm": 1.094366192817688,
    "learning_rate": 6.050420168067227e-06,
    "epoch": 2.558855885588559,
    "step": 583
  },
  {
    "loss": 2.5943,
    "grad_norm": 1.0622773170471191,
    "learning_rate": 5.7142857142857145e-06,
    "epoch": 2.563256325632563,
    "step": 584
  },
  {
    "loss": 2.7836,
    "grad_norm": 0.8318295478820801,
    "learning_rate": 5.3781512605042025e-06,
    "epoch": 2.567656765676568,
    "step": 585
  },
  {
    "loss": 2.7093,
    "grad_norm": 0.9001064300537109,
    "learning_rate": 5.04201680672269e-06,
    "epoch": 2.572057205720572,
    "step": 586
  },
  {
    "loss": 2.8985,
    "grad_norm": 1.006989598274231,
    "learning_rate": 4.705882352941177e-06,
    "epoch": 2.5764576457645765,
    "step": 587
  },
  {
    "loss": 2.666,
    "grad_norm": 1.015973687171936,
    "learning_rate": 4.369747899159664e-06,
    "epoch": 2.580858085808581,
    "step": 588
  },
  {
    "loss": 2.805,
    "grad_norm": 0.7638557553291321,
    "learning_rate": 4.033613445378152e-06,
    "epoch": 2.585258525852585,
    "step": 589
  },
  {
    "loss": 2.7417,
    "grad_norm": 0.8667567372322083,
    "learning_rate": 3.697478991596639e-06,
    "epoch": 2.5896589658965894,
    "step": 590
  },
  {
    "loss": 2.5423,
    "grad_norm": 0.9840698838233948,
    "learning_rate": 3.361344537815126e-06,
    "epoch": 2.594059405940594,
    "step": 591
  },
  {
    "loss": 2.6703,
    "grad_norm": 0.9262869954109192,
    "learning_rate": 3.0252100840336137e-06,
    "epoch": 2.5984598459845984,
    "step": 592
  },
  {
    "loss": 2.4678,
    "grad_norm": 0.883087158203125,
    "learning_rate": 2.6890756302521013e-06,
    "epoch": 2.6028602860286028,
    "step": 593
  },
  {
    "loss": 2.6753,
    "grad_norm": 0.8778444528579712,
    "learning_rate": 2.3529411764705885e-06,
    "epoch": 2.6072607260726075,
    "step": 594
  },
  {
    "loss": 2.5495,
    "grad_norm": 0.9273262619972229,
    "learning_rate": 2.016806722689076e-06,
    "epoch": 2.611661166116612,
    "step": 595
  },
  {
    "loss": 2.7604,
    "grad_norm": 1.1341376304626465,
    "learning_rate": 1.680672268907563e-06,
    "epoch": 2.616061606160616,
    "step": 596
  },
  {
    "loss": 2.7006,
    "grad_norm": 0.8621864318847656,
    "learning_rate": 1.3445378151260506e-06,
    "epoch": 2.6204620462046204,
    "step": 597
  },
  {
    "loss": 2.6753,
    "grad_norm": 1.0686204433441162,
    "learning_rate": 1.008403361344538e-06,
    "epoch": 2.6248624862486247,
    "step": 598
  },
  {
    "loss": 2.7858,
    "grad_norm": 0.8442887663841248,
    "learning_rate": 6.722689075630253e-07,
    "epoch": 2.629262926292629,
    "step": 599
  },
  {
    "loss": 2.6523,
    "grad_norm": 0.9623476266860962,
    "learning_rate": 3.3613445378151266e-07,
    "epoch": 2.633663366336634,
    "step": 600
  },
  {
    "train_runtime": 665.2809,
    "train_samples_per_second": 7.215,
    "train_steps_per_second": 0.902,
    "total_flos": 1.5929372661522432e+16,
    "train_loss": 2.863965925772985,
    "epoch": 2.633663366336634,
    "step": 600
  }
]
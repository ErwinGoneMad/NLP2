[
  {
    "loss": 3.7292,
    "grad_norm": 0.849253237247467,
    "learning_rate": 0.0,
    "epoch": 0.0044004400440044,
    "step": 1
  },
  {
    "loss": 3.8348,
    "grad_norm": 0.6212127208709717,
    "learning_rate": 4e-05,
    "epoch": 0.0088008800880088,
    "step": 2
  },
  {
    "loss": 3.896,
    "grad_norm": 0.6666445136070251,
    "learning_rate": 8e-05,
    "epoch": 0.013201320132013201,
    "step": 3
  },
  {
    "loss": 3.7202,
    "grad_norm": 0.506823718547821,
    "learning_rate": 0.00012,
    "epoch": 0.0176017601760176,
    "step": 4
  },
  {
    "loss": 3.534,
    "grad_norm": 0.6354935169219971,
    "learning_rate": 0.00016,
    "epoch": 0.022002200220022004,
    "step": 5
  },
  {
    "loss": 3.6832,
    "grad_norm": 0.4944431483745575,
    "learning_rate": 0.0002,
    "epoch": 0.026402640264026403,
    "step": 6
  },
  {
    "loss": 3.3848,
    "grad_norm": 0.5956644415855408,
    "learning_rate": 0.0001993220338983051,
    "epoch": 0.030803080308030802,
    "step": 7
  },
  {
    "loss": 3.3822,
    "grad_norm": 0.5238620638847351,
    "learning_rate": 0.00019864406779661017,
    "epoch": 0.0352035203520352,
    "step": 8
  },
  {
    "loss": 3.4023,
    "grad_norm": 0.7170441150665283,
    "learning_rate": 0.00019796610169491526,
    "epoch": 0.039603960396039604,
    "step": 9
  },
  {
    "loss": 3.3076,
    "grad_norm": 0.6136980056762695,
    "learning_rate": 0.00019728813559322035,
    "epoch": 0.04400440044004401,
    "step": 10
  },
  {
    "loss": 3.3699,
    "grad_norm": 0.6630687713623047,
    "learning_rate": 0.00019661016949152545,
    "epoch": 0.0484048404840484,
    "step": 11
  },
  {
    "loss": 3.1556,
    "grad_norm": 0.6489542722702026,
    "learning_rate": 0.0001959322033898305,
    "epoch": 0.052805280528052806,
    "step": 12
  },
  {
    "loss": 3.3704,
    "grad_norm": 0.6437230706214905,
    "learning_rate": 0.0001952542372881356,
    "epoch": 0.05720572057205721,
    "step": 13
  },
  {
    "loss": 3.2324,
    "grad_norm": 0.5125097036361694,
    "learning_rate": 0.0001945762711864407,
    "epoch": 0.061606160616061605,
    "step": 14
  },
  {
    "loss": 3.3784,
    "grad_norm": 0.5954499840736389,
    "learning_rate": 0.0001938983050847458,
    "epoch": 0.066006600660066,
    "step": 15
  },
  {
    "loss": 3.3121,
    "grad_norm": 0.5921770930290222,
    "learning_rate": 0.00019322033898305085,
    "epoch": 0.0704070407040704,
    "step": 16
  },
  {
    "loss": 3.0025,
    "grad_norm": 0.5591530203819275,
    "learning_rate": 0.00019254237288135595,
    "epoch": 0.0748074807480748,
    "step": 17
  },
  {
    "loss": 3.1123,
    "grad_norm": 0.5259161591529846,
    "learning_rate": 0.000191864406779661,
    "epoch": 0.07920792079207921,
    "step": 18
  },
  {
    "loss": 3.2243,
    "grad_norm": 0.5364188551902771,
    "learning_rate": 0.0001911864406779661,
    "epoch": 0.08360836083608361,
    "step": 19
  },
  {
    "loss": 3.2081,
    "grad_norm": 0.5777730941772461,
    "learning_rate": 0.0001905084745762712,
    "epoch": 0.08800880088008801,
    "step": 20
  },
  {
    "loss": 3.2964,
    "grad_norm": 0.5501466989517212,
    "learning_rate": 0.0001898305084745763,
    "epoch": 0.0924092409240924,
    "step": 21
  },
  {
    "loss": 3.3105,
    "grad_norm": 0.4652327597141266,
    "learning_rate": 0.00018915254237288136,
    "epoch": 0.0968096809680968,
    "step": 22
  },
  {
    "loss": 3.1684,
    "grad_norm": 0.5063382387161255,
    "learning_rate": 0.00018847457627118645,
    "epoch": 0.10121012101210121,
    "step": 23
  },
  {
    "loss": 3.1593,
    "grad_norm": 0.5682255625724792,
    "learning_rate": 0.00018779661016949151,
    "epoch": 0.10561056105610561,
    "step": 24
  },
  {
    "loss": 3.2922,
    "grad_norm": 0.5402249097824097,
    "learning_rate": 0.00018711864406779663,
    "epoch": 0.11001100110011001,
    "step": 25
  },
  {
    "loss": 3.3715,
    "grad_norm": 0.5273281931877136,
    "learning_rate": 0.0001864406779661017,
    "epoch": 0.11441144114411442,
    "step": 26
  },
  {
    "loss": 3.4273,
    "grad_norm": 0.6027575135231018,
    "learning_rate": 0.0001857627118644068,
    "epoch": 0.1188118811881188,
    "step": 27
  },
  {
    "loss": 3.0944,
    "grad_norm": 0.47241368889808655,
    "learning_rate": 0.00018508474576271186,
    "epoch": 0.12321232123212321,
    "step": 28
  },
  {
    "loss": 2.8579,
    "grad_norm": 0.5095050930976868,
    "learning_rate": 0.00018440677966101695,
    "epoch": 0.1276127612761276,
    "step": 29
  },
  {
    "loss": 3.2724,
    "grad_norm": 0.48835739493370056,
    "learning_rate": 0.00018372881355932204,
    "epoch": 0.132013201320132,
    "step": 30
  },
  {
    "loss": 3.1901,
    "grad_norm": 0.49152860045433044,
    "learning_rate": 0.00018305084745762714,
    "epoch": 0.13641364136413642,
    "step": 31
  },
  {
    "loss": 2.9839,
    "grad_norm": 0.4814511239528656,
    "learning_rate": 0.0001823728813559322,
    "epoch": 0.1408140814081408,
    "step": 32
  },
  {
    "loss": 3.2251,
    "grad_norm": 0.5631855726242065,
    "learning_rate": 0.0001816949152542373,
    "epoch": 0.14521452145214522,
    "step": 33
  },
  {
    "loss": 3.1912,
    "grad_norm": 0.5235497951507568,
    "learning_rate": 0.00018101694915254239,
    "epoch": 0.1496149614961496,
    "step": 34
  },
  {
    "loss": 3.1373,
    "grad_norm": 0.5196405649185181,
    "learning_rate": 0.00018033898305084748,
    "epoch": 0.15401540154015403,
    "step": 35
  },
  {
    "loss": 3.2787,
    "grad_norm": 0.5527157187461853,
    "learning_rate": 0.00017966101694915257,
    "epoch": 0.15841584158415842,
    "step": 36
  },
  {
    "loss": 3.1246,
    "grad_norm": 0.5510281920433044,
    "learning_rate": 0.00017898305084745764,
    "epoch": 0.1628162816281628,
    "step": 37
  },
  {
    "loss": 3.0454,
    "grad_norm": 0.6366437077522278,
    "learning_rate": 0.00017830508474576273,
    "epoch": 0.16721672167216722,
    "step": 38
  },
  {
    "loss": 3.0774,
    "grad_norm": 0.6225066781044006,
    "learning_rate": 0.0001776271186440678,
    "epoch": 0.1716171617161716,
    "step": 39
  },
  {
    "loss": 2.9774,
    "grad_norm": 0.6104570627212524,
    "learning_rate": 0.0001769491525423729,
    "epoch": 0.17601760176017603,
    "step": 40
  },
  {
    "loss": 3.1363,
    "grad_norm": 0.5186161994934082,
    "learning_rate": 0.00017627118644067798,
    "epoch": 0.18041804180418042,
    "step": 41
  },
  {
    "loss": 3.0756,
    "grad_norm": 0.570706307888031,
    "learning_rate": 0.00017559322033898307,
    "epoch": 0.1848184818481848,
    "step": 42
  },
  {
    "loss": 2.9238,
    "grad_norm": 0.5943261981010437,
    "learning_rate": 0.00017491525423728814,
    "epoch": 0.18921892189218922,
    "step": 43
  },
  {
    "loss": 3.262,
    "grad_norm": 0.5742101073265076,
    "learning_rate": 0.00017423728813559323,
    "epoch": 0.1936193619361936,
    "step": 44
  },
  {
    "loss": 3.1163,
    "grad_norm": 0.5960097908973694,
    "learning_rate": 0.0001735593220338983,
    "epoch": 0.19801980198019803,
    "step": 45
  },
  {
    "loss": 3.1849,
    "grad_norm": 0.5213925838470459,
    "learning_rate": 0.00017288135593220342,
    "epoch": 0.20242024202420242,
    "step": 46
  },
  {
    "loss": 3.0396,
    "grad_norm": 0.7673141360282898,
    "learning_rate": 0.00017220338983050848,
    "epoch": 0.2068206820682068,
    "step": 47
  },
  {
    "loss": 3.2023,
    "grad_norm": 0.5607529878616333,
    "learning_rate": 0.00017152542372881357,
    "epoch": 0.21122112211221122,
    "step": 48
  },
  {
    "loss": 3.0122,
    "grad_norm": 0.5466693043708801,
    "learning_rate": 0.00017084745762711864,
    "epoch": 0.2156215621562156,
    "step": 49
  },
  {
    "loss": 3.2137,
    "grad_norm": 0.6949008703231812,
    "learning_rate": 0.00017016949152542373,
    "epoch": 0.22002200220022003,
    "step": 50
  },
  {
    "loss": 2.8131,
    "grad_norm": 0.6364690065383911,
    "learning_rate": 0.00016949152542372882,
    "epoch": 0.22442244224422442,
    "step": 51
  },
  {
    "loss": 3.0277,
    "grad_norm": 0.6545647978782654,
    "learning_rate": 0.00016881355932203392,
    "epoch": 0.22882288228822883,
    "step": 52
  },
  {
    "loss": 3.2098,
    "grad_norm": 0.7596540451049805,
    "learning_rate": 0.00016813559322033898,
    "epoch": 0.23322332233223322,
    "step": 53
  },
  {
    "loss": 3.0532,
    "grad_norm": 0.6868033409118652,
    "learning_rate": 0.00016745762711864408,
    "epoch": 0.2376237623762376,
    "step": 54
  },
  {
    "loss": 3.0433,
    "grad_norm": 0.6078927516937256,
    "learning_rate": 0.00016677966101694914,
    "epoch": 0.24202420242024203,
    "step": 55
  },
  {
    "loss": 3.1815,
    "grad_norm": 0.6416008472442627,
    "learning_rate": 0.00016610169491525423,
    "epoch": 0.24642464246424642,
    "step": 56
  },
  {
    "loss": 3.033,
    "grad_norm": 0.6445062160491943,
    "learning_rate": 0.00016542372881355933,
    "epoch": 0.2508250825082508,
    "step": 57
  },
  {
    "loss": 3.1332,
    "grad_norm": 0.5327661037445068,
    "learning_rate": 0.00016474576271186442,
    "epoch": 0.2552255225522552,
    "step": 58
  },
  {
    "loss": 3.1168,
    "grad_norm": 0.6067260503768921,
    "learning_rate": 0.00016406779661016948,
    "epoch": 0.25962596259625964,
    "step": 59
  },
  {
    "loss": 3.1741,
    "grad_norm": 0.6921758055686951,
    "learning_rate": 0.00016338983050847458,
    "epoch": 0.264026402640264,
    "step": 60
  },
  {
    "loss": 3.1378,
    "grad_norm": 0.56170654296875,
    "learning_rate": 0.00016271186440677967,
    "epoch": 0.2684268426842684,
    "step": 61
  },
  {
    "loss": 3.1238,
    "grad_norm": 0.5201604962348938,
    "learning_rate": 0.00016203389830508476,
    "epoch": 0.27282728272827284,
    "step": 62
  },
  {
    "loss": 3.0501,
    "grad_norm": 0.7121450304985046,
    "learning_rate": 0.00016135593220338985,
    "epoch": 0.27722772277227725,
    "step": 63
  },
  {
    "loss": 3.1824,
    "grad_norm": 0.6076852679252625,
    "learning_rate": 0.00016067796610169492,
    "epoch": 0.2816281628162816,
    "step": 64
  },
  {
    "loss": 2.9793,
    "grad_norm": 0.5883669257164001,
    "learning_rate": 0.00016,
    "epoch": 0.28602860286028603,
    "step": 65
  },
  {
    "loss": 3.1145,
    "grad_norm": 0.559769332408905,
    "learning_rate": 0.00015932203389830508,
    "epoch": 0.29042904290429045,
    "step": 66
  },
  {
    "loss": 3.0566,
    "grad_norm": 0.5882605314254761,
    "learning_rate": 0.0001586440677966102,
    "epoch": 0.2948294829482948,
    "step": 67
  },
  {
    "loss": 2.9869,
    "grad_norm": 0.6885153651237488,
    "learning_rate": 0.00015796610169491526,
    "epoch": 0.2992299229922992,
    "step": 68
  },
  {
    "loss": 2.9109,
    "grad_norm": 0.6247542500495911,
    "learning_rate": 0.00015728813559322036,
    "epoch": 0.30363036303630364,
    "step": 69
  },
  {
    "loss": 3.1442,
    "grad_norm": 0.5864186882972717,
    "learning_rate": 0.00015661016949152542,
    "epoch": 0.30803080308030806,
    "step": 70
  },
  {
    "loss": 3.2106,
    "grad_norm": 0.7592739462852478,
    "learning_rate": 0.00015593220338983051,
    "epoch": 0.3124312431243124,
    "step": 71
  },
  {
    "loss": 3.1478,
    "grad_norm": 0.6463990211486816,
    "learning_rate": 0.0001552542372881356,
    "epoch": 0.31683168316831684,
    "step": 72
  },
  {
    "loss": 3.2383,
    "grad_norm": 0.8923975229263306,
    "learning_rate": 0.0001545762711864407,
    "epoch": 0.32123212321232125,
    "step": 73
  },
  {
    "loss": 2.9659,
    "grad_norm": 0.6455633044242859,
    "learning_rate": 0.00015389830508474577,
    "epoch": 0.3256325632563256,
    "step": 74
  },
  {
    "loss": 2.9917,
    "grad_norm": 0.695464551448822,
    "learning_rate": 0.00015322033898305086,
    "epoch": 0.33003300330033003,
    "step": 75
  },
  {
    "loss": 3.0489,
    "grad_norm": 0.7750329971313477,
    "learning_rate": 0.00015254237288135592,
    "epoch": 0.33443344334433445,
    "step": 76
  },
  {
    "loss": 2.9673,
    "grad_norm": 0.6707773208618164,
    "learning_rate": 0.00015186440677966102,
    "epoch": 0.3388338833883388,
    "step": 77
  },
  {
    "loss": 3.2319,
    "grad_norm": 0.7039449214935303,
    "learning_rate": 0.0001511864406779661,
    "epoch": 0.3432343234323432,
    "step": 78
  },
  {
    "loss": 3.0626,
    "grad_norm": 0.7849270105361938,
    "learning_rate": 0.0001505084745762712,
    "epoch": 0.34763476347634764,
    "step": 79
  },
  {
    "loss": 3.2115,
    "grad_norm": 0.6575201749801636,
    "learning_rate": 0.00014983050847457627,
    "epoch": 0.35203520352035206,
    "step": 80
  },
  {
    "loss": 2.9595,
    "grad_norm": 0.662661612033844,
    "learning_rate": 0.00014915254237288136,
    "epoch": 0.3564356435643564,
    "step": 81
  },
  {
    "loss": 3.1842,
    "grad_norm": 0.7877310514450073,
    "learning_rate": 0.00014847457627118645,
    "epoch": 0.36083608360836084,
    "step": 82
  },
  {
    "loss": 3.4206,
    "grad_norm": 0.8294408321380615,
    "learning_rate": 0.00014779661016949154,
    "epoch": 0.36523652365236525,
    "step": 83
  },
  {
    "loss": 3.1549,
    "grad_norm": 0.7002351880073547,
    "learning_rate": 0.0001471186440677966,
    "epoch": 0.3696369636963696,
    "step": 84
  },
  {
    "loss": 2.9446,
    "grad_norm": 0.8196422457695007,
    "learning_rate": 0.0001464406779661017,
    "epoch": 0.37403740374037403,
    "step": 85
  },
  {
    "loss": 2.9063,
    "grad_norm": 0.6021637916564941,
    "learning_rate": 0.00014576271186440677,
    "epoch": 0.37843784378437845,
    "step": 86
  },
  {
    "loss": 3.0529,
    "grad_norm": 0.666060209274292,
    "learning_rate": 0.00014508474576271186,
    "epoch": 0.38283828382838286,
    "step": 87
  },
  {
    "loss": 3.0784,
    "grad_norm": 0.6988681554794312,
    "learning_rate": 0.00014440677966101695,
    "epoch": 0.3872387238723872,
    "step": 88
  },
  {
    "loss": 3.1962,
    "grad_norm": 0.6874337792396545,
    "learning_rate": 0.00014372881355932205,
    "epoch": 0.39163916391639164,
    "step": 89
  },
  {
    "loss": 3.019,
    "grad_norm": 0.9402799606323242,
    "learning_rate": 0.00014305084745762714,
    "epoch": 0.39603960396039606,
    "step": 90
  },
  {
    "loss": 2.9326,
    "grad_norm": 0.746972382068634,
    "learning_rate": 0.0001423728813559322,
    "epoch": 0.4004400440044004,
    "step": 91
  },
  {
    "loss": 3.1161,
    "grad_norm": 0.8672400712966919,
    "learning_rate": 0.0001416949152542373,
    "epoch": 0.40484048404840484,
    "step": 92
  },
  {
    "loss": 2.9156,
    "grad_norm": 0.9852327704429626,
    "learning_rate": 0.0001410169491525424,
    "epoch": 0.40924092409240925,
    "step": 93
  },
  {
    "loss": 2.9998,
    "grad_norm": 0.6898754239082336,
    "learning_rate": 0.00014033898305084748,
    "epoch": 0.4136413641364136,
    "step": 94
  },
  {
    "loss": 2.9797,
    "grad_norm": 0.7573415637016296,
    "learning_rate": 0.00013966101694915255,
    "epoch": 0.41804180418041803,
    "step": 95
  },
  {
    "loss": 2.9153,
    "grad_norm": 0.6701633930206299,
    "learning_rate": 0.00013898305084745764,
    "epoch": 0.42244224422442245,
    "step": 96
  },
  {
    "loss": 2.9765,
    "grad_norm": 0.705197274684906,
    "learning_rate": 0.0001383050847457627,
    "epoch": 0.42684268426842686,
    "step": 97
  },
  {
    "loss": 2.9261,
    "grad_norm": 0.6696965098381042,
    "learning_rate": 0.0001376271186440678,
    "epoch": 0.4312431243124312,
    "step": 98
  },
  {
    "loss": 3.068,
    "grad_norm": 0.6875851154327393,
    "learning_rate": 0.0001369491525423729,
    "epoch": 0.43564356435643564,
    "step": 99
  },
  {
    "loss": 3.2965,
    "grad_norm": 0.9220380187034607,
    "learning_rate": 0.00013627118644067798,
    "epoch": 0.44004400440044006,
    "step": 100
  },
  {
    "loss": 2.9961,
    "grad_norm": 0.5977919697761536,
    "learning_rate": 0.00013559322033898305,
    "epoch": 0.4444444444444444,
    "step": 101
  },
  {
    "loss": 3.0511,
    "grad_norm": 0.9442690014839172,
    "learning_rate": 0.00013491525423728814,
    "epoch": 0.44884488448844884,
    "step": 102
  },
  {
    "loss": 3.063,
    "grad_norm": 0.9654199481010437,
    "learning_rate": 0.0001342372881355932,
    "epoch": 0.45324532453245325,
    "step": 103
  },
  {
    "loss": 2.975,
    "grad_norm": 0.8741172552108765,
    "learning_rate": 0.00013355932203389833,
    "epoch": 0.45764576457645767,
    "step": 104
  },
  {
    "loss": 2.9482,
    "grad_norm": 0.7269368171691895,
    "learning_rate": 0.0001328813559322034,
    "epoch": 0.46204620462046203,
    "step": 105
  },
  {
    "loss": 3.1051,
    "grad_norm": 0.6508907079696655,
    "learning_rate": 0.00013220338983050849,
    "epoch": 0.46644664466446645,
    "step": 106
  },
  {
    "loss": 3.0086,
    "grad_norm": 0.7307413816452026,
    "learning_rate": 0.00013152542372881355,
    "epoch": 0.47084708470847086,
    "step": 107
  },
  {
    "loss": 2.9439,
    "grad_norm": 0.7309480905532837,
    "learning_rate": 0.00013084745762711864,
    "epoch": 0.4752475247524752,
    "step": 108
  },
  {
    "loss": 2.98,
    "grad_norm": 0.7421491742134094,
    "learning_rate": 0.00013016949152542374,
    "epoch": 0.47964796479647964,
    "step": 109
  },
  {
    "loss": 3.078,
    "grad_norm": 0.64533531665802,
    "learning_rate": 0.00012949152542372883,
    "epoch": 0.48404840484048406,
    "step": 110
  },
  {
    "loss": 3.2192,
    "grad_norm": 0.6661964058876038,
    "learning_rate": 0.0001288135593220339,
    "epoch": 0.4884488448844885,
    "step": 111
  },
  {
    "loss": 2.8762,
    "grad_norm": 0.6509435176849365,
    "learning_rate": 0.000128135593220339,
    "epoch": 0.49284928492849284,
    "step": 112
  },
  {
    "loss": 2.9975,
    "grad_norm": 0.8055919408798218,
    "learning_rate": 0.00012745762711864405,
    "epoch": 0.49724972497249725,
    "step": 113
  },
  {
    "loss": 3.045,
    "grad_norm": 0.7131739854812622,
    "learning_rate": 0.00012677966101694917,
    "epoch": 0.5016501650165016,
    "step": 114
  },
  {
    "loss": 3.0761,
    "grad_norm": 0.7547053098678589,
    "learning_rate": 0.00012610169491525426,
    "epoch": 0.506050605060506,
    "step": 115
  },
  {
    "loss": 2.9768,
    "grad_norm": 0.7528958320617676,
    "learning_rate": 0.00012542372881355933,
    "epoch": 0.5104510451045104,
    "step": 116
  },
  {
    "loss": 2.923,
    "grad_norm": 0.7141650915145874,
    "learning_rate": 0.00012474576271186442,
    "epoch": 0.5148514851485149,
    "step": 117
  },
  {
    "loss": 2.9808,
    "grad_norm": 0.7093183994293213,
    "learning_rate": 0.0001240677966101695,
    "epoch": 0.5192519251925193,
    "step": 118
  },
  {
    "loss": 3.0243,
    "grad_norm": 0.8201227784156799,
    "learning_rate": 0.00012338983050847458,
    "epoch": 0.5236523652365237,
    "step": 119
  },
  {
    "loss": 3.0008,
    "grad_norm": 0.698085606098175,
    "learning_rate": 0.00012271186440677967,
    "epoch": 0.528052805280528,
    "step": 120
  },
  {
    "loss": 3.0943,
    "grad_norm": 0.7823898196220398,
    "learning_rate": 0.00012203389830508477,
    "epoch": 0.5324532453245324,
    "step": 121
  },
  {
    "loss": 2.9363,
    "grad_norm": 0.6378811597824097,
    "learning_rate": 0.00012135593220338983,
    "epoch": 0.5368536853685368,
    "step": 122
  },
  {
    "loss": 2.9472,
    "grad_norm": 0.8096580505371094,
    "learning_rate": 0.00012067796610169492,
    "epoch": 0.5412541254125413,
    "step": 123
  },
  {
    "loss": 2.99,
    "grad_norm": 0.8135790824890137,
    "learning_rate": 0.00012,
    "epoch": 0.5456545654565457,
    "step": 124
  },
  {
    "loss": 3.1499,
    "grad_norm": 0.6139174103736877,
    "learning_rate": 0.0001193220338983051,
    "epoch": 0.5500550055005501,
    "step": 125
  },
  {
    "loss": 2.8194,
    "grad_norm": 0.8690942525863647,
    "learning_rate": 0.00011864406779661017,
    "epoch": 0.5544554455445545,
    "step": 126
  },
  {
    "loss": 2.9883,
    "grad_norm": 0.8137745261192322,
    "learning_rate": 0.00011796610169491527,
    "epoch": 0.5588558855885588,
    "step": 127
  },
  {
    "loss": 2.9538,
    "grad_norm": 0.6995993852615356,
    "learning_rate": 0.00011728813559322033,
    "epoch": 0.5632563256325632,
    "step": 128
  },
  {
    "loss": 2.9459,
    "grad_norm": 0.7925785183906555,
    "learning_rate": 0.00011661016949152544,
    "epoch": 0.5676567656765676,
    "step": 129
  },
  {
    "loss": 2.94,
    "grad_norm": 0.7169944047927856,
    "learning_rate": 0.0001159322033898305,
    "epoch": 0.5720572057205721,
    "step": 130
  },
  {
    "loss": 2.8419,
    "grad_norm": 0.7065644860267639,
    "learning_rate": 0.0001152542372881356,
    "epoch": 0.5764576457645765,
    "step": 131
  },
  {
    "loss": 3.1884,
    "grad_norm": 0.8154429197311401,
    "learning_rate": 0.00011457627118644068,
    "epoch": 0.5808580858085809,
    "step": 132
  },
  {
    "loss": 3.0955,
    "grad_norm": 0.7171697020530701,
    "learning_rate": 0.00011389830508474577,
    "epoch": 0.5852585258525853,
    "step": 133
  },
  {
    "loss": 2.8909,
    "grad_norm": 0.8650662899017334,
    "learning_rate": 0.00011322033898305085,
    "epoch": 0.5896589658965896,
    "step": 134
  },
  {
    "loss": 3.1108,
    "grad_norm": 0.6193032264709473,
    "learning_rate": 0.00011254237288135594,
    "epoch": 0.594059405940594,
    "step": 135
  },
  {
    "loss": 2.9247,
    "grad_norm": 0.5973473787307739,
    "learning_rate": 0.00011186440677966102,
    "epoch": 0.5984598459845984,
    "step": 136
  },
  {
    "loss": 3.1849,
    "grad_norm": 0.6180147528648376,
    "learning_rate": 0.00011118644067796611,
    "epoch": 0.6028602860286029,
    "step": 137
  },
  {
    "loss": 3.0571,
    "grad_norm": 0.7763093709945679,
    "learning_rate": 0.00011050847457627118,
    "epoch": 0.6072607260726073,
    "step": 138
  },
  {
    "loss": 3.0107,
    "grad_norm": 0.8039466738700867,
    "learning_rate": 0.00010983050847457627,
    "epoch": 0.6116611661166117,
    "step": 139
  },
  {
    "loss": 2.9093,
    "grad_norm": 0.798930287361145,
    "learning_rate": 0.00010915254237288135,
    "epoch": 0.6160616061606161,
    "step": 140
  },
  {
    "loss": 3.0065,
    "grad_norm": 0.670403242111206,
    "learning_rate": 0.00010847457627118644,
    "epoch": 0.6204620462046204,
    "step": 141
  },
  {
    "loss": 3.0759,
    "grad_norm": 0.6473331451416016,
    "learning_rate": 0.00010779661016949153,
    "epoch": 0.6248624862486248,
    "step": 142
  },
  {
    "loss": 3.1502,
    "grad_norm": 0.6236024498939514,
    "learning_rate": 0.00010711864406779661,
    "epoch": 0.6292629262926293,
    "step": 143
  },
  {
    "loss": 2.9652,
    "grad_norm": 0.7157099843025208,
    "learning_rate": 0.0001064406779661017,
    "epoch": 0.6336633663366337,
    "step": 144
  },
  {
    "loss": 2.8997,
    "grad_norm": 0.7197355628013611,
    "learning_rate": 0.00010576271186440679,
    "epoch": 0.6380638063806381,
    "step": 145
  },
  {
    "loss": 2.7307,
    "grad_norm": 0.6758115291595459,
    "learning_rate": 0.00010508474576271188,
    "epoch": 0.6424642464246425,
    "step": 146
  },
  {
    "loss": 2.9359,
    "grad_norm": 0.6946230530738831,
    "learning_rate": 0.00010440677966101696,
    "epoch": 0.6468646864686468,
    "step": 147
  },
  {
    "loss": 3.0219,
    "grad_norm": 0.7815505266189575,
    "learning_rate": 0.00010372881355932205,
    "epoch": 0.6512651265126512,
    "step": 148
  },
  {
    "loss": 2.8908,
    "grad_norm": 0.7472509741783142,
    "learning_rate": 0.00010305084745762712,
    "epoch": 0.6556655665566556,
    "step": 149
  },
  {
    "loss": 2.8731,
    "grad_norm": 0.741915225982666,
    "learning_rate": 0.00010237288135593222,
    "epoch": 0.6600660066006601,
    "step": 150
  },
  {
    "loss": 3.0415,
    "grad_norm": 0.713412880897522,
    "learning_rate": 0.00010169491525423729,
    "epoch": 0.6644664466446645,
    "step": 151
  },
  {
    "loss": 2.8804,
    "grad_norm": 0.8068578243255615,
    "learning_rate": 0.00010101694915254238,
    "epoch": 0.6688668866886689,
    "step": 152
  },
  {
    "loss": 2.789,
    "grad_norm": 0.874701976776123,
    "learning_rate": 0.00010033898305084746,
    "epoch": 0.6732673267326733,
    "step": 153
  },
  {
    "loss": 2.9029,
    "grad_norm": 0.7080183625221252,
    "learning_rate": 9.966101694915255e-05,
    "epoch": 0.6776677667766776,
    "step": 154
  },
  {
    "loss": 3.0666,
    "grad_norm": 0.776543915271759,
    "learning_rate": 9.898305084745763e-05,
    "epoch": 0.682068206820682,
    "step": 155
  },
  {
    "loss": 2.8838,
    "grad_norm": 0.755859911441803,
    "learning_rate": 9.830508474576272e-05,
    "epoch": 0.6864686468646864,
    "step": 156
  },
  {
    "loss": 3.0777,
    "grad_norm": 0.6087724566459656,
    "learning_rate": 9.76271186440678e-05,
    "epoch": 0.6908690869086909,
    "step": 157
  },
  {
    "loss": 3.0328,
    "grad_norm": 0.6356218457221985,
    "learning_rate": 9.69491525423729e-05,
    "epoch": 0.6952695269526953,
    "step": 158
  },
  {
    "loss": 2.876,
    "grad_norm": 0.7451726794242859,
    "learning_rate": 9.627118644067797e-05,
    "epoch": 0.6996699669966997,
    "step": 159
  },
  {
    "loss": 3.0272,
    "grad_norm": 0.7003920674324036,
    "learning_rate": 9.559322033898305e-05,
    "epoch": 0.7040704070407041,
    "step": 160
  },
  {
    "loss": 3.0444,
    "grad_norm": 0.7594775557518005,
    "learning_rate": 9.491525423728815e-05,
    "epoch": 0.7084708470847084,
    "step": 161
  },
  {
    "loss": 3.1191,
    "grad_norm": 0.7243688106536865,
    "learning_rate": 9.423728813559322e-05,
    "epoch": 0.7128712871287128,
    "step": 162
  },
  {
    "loss": 2.9007,
    "grad_norm": 0.6263774037361145,
    "learning_rate": 9.355932203389832e-05,
    "epoch": 0.7172717271727173,
    "step": 163
  },
  {
    "loss": 2.9213,
    "grad_norm": 0.7415355443954468,
    "learning_rate": 9.28813559322034e-05,
    "epoch": 0.7216721672167217,
    "step": 164
  },
  {
    "loss": 2.921,
    "grad_norm": 0.6925292015075684,
    "learning_rate": 9.220338983050847e-05,
    "epoch": 0.7260726072607261,
    "step": 165
  },
  {
    "loss": 2.9046,
    "grad_norm": 0.8176233172416687,
    "learning_rate": 9.152542372881357e-05,
    "epoch": 0.7304730473047305,
    "step": 166
  },
  {
    "loss": 2.9951,
    "grad_norm": 0.9134089946746826,
    "learning_rate": 9.084745762711865e-05,
    "epoch": 0.7348734873487349,
    "step": 167
  },
  {
    "loss": 2.9497,
    "grad_norm": 0.6757708191871643,
    "learning_rate": 9.016949152542374e-05,
    "epoch": 0.7392739273927392,
    "step": 168
  },
  {
    "loss": 2.5988,
    "grad_norm": 0.8995665907859802,
    "learning_rate": 8.949152542372882e-05,
    "epoch": 0.7436743674367436,
    "step": 169
  },
  {
    "loss": 3.1298,
    "grad_norm": 0.6704635620117188,
    "learning_rate": 8.88135593220339e-05,
    "epoch": 0.7480748074807481,
    "step": 170
  },
  {
    "loss": 2.9538,
    "grad_norm": 0.6709250211715698,
    "learning_rate": 8.813559322033899e-05,
    "epoch": 0.7524752475247525,
    "step": 171
  },
  {
    "loss": 2.9134,
    "grad_norm": 0.8385018110275269,
    "learning_rate": 8.745762711864407e-05,
    "epoch": 0.7568756875687569,
    "step": 172
  },
  {
    "loss": 3.0665,
    "grad_norm": 0.7465764284133911,
    "learning_rate": 8.677966101694915e-05,
    "epoch": 0.7612761276127613,
    "step": 173
  },
  {
    "loss": 2.9796,
    "grad_norm": 0.7740926742553711,
    "learning_rate": 8.610169491525424e-05,
    "epoch": 0.7656765676567657,
    "step": 174
  },
  {
    "loss": 3.0163,
    "grad_norm": 0.7445359826087952,
    "learning_rate": 8.542372881355932e-05,
    "epoch": 0.77007700770077,
    "step": 175
  },
  {
    "loss": 2.8736,
    "grad_norm": 0.6014329791069031,
    "learning_rate": 8.474576271186441e-05,
    "epoch": 0.7744774477447744,
    "step": 176
  },
  {
    "loss": 3.0635,
    "grad_norm": 0.5849364399909973,
    "learning_rate": 8.406779661016949e-05,
    "epoch": 0.7788778877887789,
    "step": 177
  },
  {
    "loss": 2.9426,
    "grad_norm": 0.6585785150527954,
    "learning_rate": 8.338983050847457e-05,
    "epoch": 0.7832783278327833,
    "step": 178
  },
  {
    "loss": 3.1446,
    "grad_norm": 0.9713262319564819,
    "learning_rate": 8.271186440677966e-05,
    "epoch": 0.7876787678767877,
    "step": 179
  },
  {
    "loss": 3.2206,
    "grad_norm": 0.709604024887085,
    "learning_rate": 8.203389830508474e-05,
    "epoch": 0.7920792079207921,
    "step": 180
  },
  {
    "loss": 2.9952,
    "grad_norm": 0.8150402307510376,
    "learning_rate": 8.135593220338983e-05,
    "epoch": 0.7964796479647965,
    "step": 181
  },
  {
    "loss": 2.8822,
    "grad_norm": 0.6203869581222534,
    "learning_rate": 8.067796610169493e-05,
    "epoch": 0.8008800880088008,
    "step": 182
  },
  {
    "loss": 2.9196,
    "grad_norm": 0.7894602417945862,
    "learning_rate": 8e-05,
    "epoch": 0.8052805280528053,
    "step": 183
  },
  {
    "loss": 2.9195,
    "grad_norm": 0.7774238586425781,
    "learning_rate": 7.93220338983051e-05,
    "epoch": 0.8096809680968097,
    "step": 184
  },
  {
    "loss": 3.0225,
    "grad_norm": 0.7265953421592712,
    "learning_rate": 7.864406779661018e-05,
    "epoch": 0.8140814081408141,
    "step": 185
  },
  {
    "loss": 2.9343,
    "grad_norm": 0.7410013675689697,
    "learning_rate": 7.796610169491526e-05,
    "epoch": 0.8184818481848185,
    "step": 186
  },
  {
    "loss": 2.8844,
    "grad_norm": 0.6694686412811279,
    "learning_rate": 7.728813559322035e-05,
    "epoch": 0.8228822882288229,
    "step": 187
  },
  {
    "loss": 2.9713,
    "grad_norm": 0.9263887405395508,
    "learning_rate": 7.661016949152543e-05,
    "epoch": 0.8272827282728272,
    "step": 188
  },
  {
    "loss": 3.2557,
    "grad_norm": 0.8087826371192932,
    "learning_rate": 7.593220338983051e-05,
    "epoch": 0.8316831683168316,
    "step": 189
  },
  {
    "loss": 2.8135,
    "grad_norm": 0.6985583901405334,
    "learning_rate": 7.52542372881356e-05,
    "epoch": 0.8360836083608361,
    "step": 190
  },
  {
    "loss": 2.9829,
    "grad_norm": 0.7247896790504456,
    "learning_rate": 7.457627118644068e-05,
    "epoch": 0.8404840484048405,
    "step": 191
  },
  {
    "loss": 2.8819,
    "grad_norm": 0.8084210157394409,
    "learning_rate": 7.389830508474577e-05,
    "epoch": 0.8448844884488449,
    "step": 192
  },
  {
    "loss": 3.0991,
    "grad_norm": 0.6928616166114807,
    "learning_rate": 7.322033898305085e-05,
    "epoch": 0.8492849284928493,
    "step": 193
  },
  {
    "loss": 2.8791,
    "grad_norm": 0.7218028903007507,
    "learning_rate": 7.254237288135593e-05,
    "epoch": 0.8536853685368537,
    "step": 194
  },
  {
    "loss": 3.0667,
    "grad_norm": 0.656561017036438,
    "learning_rate": 7.186440677966102e-05,
    "epoch": 0.858085808580858,
    "step": 195
  },
  {
    "loss": 3.0523,
    "grad_norm": 0.6625403761863708,
    "learning_rate": 7.11864406779661e-05,
    "epoch": 0.8624862486248625,
    "step": 196
  },
  {
    "loss": 3.0073,
    "grad_norm": 0.7216251492500305,
    "learning_rate": 7.05084745762712e-05,
    "epoch": 0.8668866886688669,
    "step": 197
  },
  {
    "loss": 2.9763,
    "grad_norm": 0.6880885362625122,
    "learning_rate": 6.983050847457627e-05,
    "epoch": 0.8712871287128713,
    "step": 198
  },
  {
    "loss": 3.2008,
    "grad_norm": 0.7174074053764343,
    "learning_rate": 6.915254237288135e-05,
    "epoch": 0.8756875687568757,
    "step": 199
  },
  {
    "loss": 2.9391,
    "grad_norm": 0.7548096776008606,
    "learning_rate": 6.847457627118645e-05,
    "epoch": 0.8800880088008801,
    "step": 200
  },
  {
    "loss": 2.7745,
    "grad_norm": 0.7148354053497314,
    "learning_rate": 6.779661016949152e-05,
    "epoch": 0.8844884488448845,
    "step": 201
  },
  {
    "loss": 2.8435,
    "grad_norm": 0.812374472618103,
    "learning_rate": 6.71186440677966e-05,
    "epoch": 0.8888888888888888,
    "step": 202
  },
  {
    "loss": 3.0755,
    "grad_norm": 0.8175057768821716,
    "learning_rate": 6.64406779661017e-05,
    "epoch": 0.8932893289328933,
    "step": 203
  },
  {
    "loss": 2.7837,
    "grad_norm": 0.698677122592926,
    "learning_rate": 6.576271186440678e-05,
    "epoch": 0.8976897689768977,
    "step": 204
  },
  {
    "loss": 2.8368,
    "grad_norm": 0.7561160326004028,
    "learning_rate": 6.508474576271187e-05,
    "epoch": 0.9020902090209021,
    "step": 205
  },
  {
    "loss": 2.7593,
    "grad_norm": 0.6829096078872681,
    "learning_rate": 6.440677966101695e-05,
    "epoch": 0.9064906490649065,
    "step": 206
  },
  {
    "loss": 2.8809,
    "grad_norm": 0.816378116607666,
    "learning_rate": 6.372881355932203e-05,
    "epoch": 0.9108910891089109,
    "step": 207
  },
  {
    "loss": 3.0014,
    "grad_norm": 0.8721187114715576,
    "learning_rate": 6.305084745762713e-05,
    "epoch": 0.9152915291529153,
    "step": 208
  },
  {
    "loss": 2.9621,
    "grad_norm": 0.8065148591995239,
    "learning_rate": 6.237288135593221e-05,
    "epoch": 0.9196919691969196,
    "step": 209
  },
  {
    "loss": 3.0108,
    "grad_norm": 0.7237281203269958,
    "learning_rate": 6.169491525423729e-05,
    "epoch": 0.9240924092409241,
    "step": 210
  },
  {
    "loss": 2.8043,
    "grad_norm": 0.736205518245697,
    "learning_rate": 6.101694915254238e-05,
    "epoch": 0.9284928492849285,
    "step": 211
  },
  {
    "loss": 2.9004,
    "grad_norm": 0.6571990847587585,
    "learning_rate": 6.033898305084746e-05,
    "epoch": 0.9328932893289329,
    "step": 212
  },
  {
    "loss": 3.0912,
    "grad_norm": 0.8627695441246033,
    "learning_rate": 5.966101694915255e-05,
    "epoch": 0.9372937293729373,
    "step": 213
  },
  {
    "loss": 2.8644,
    "grad_norm": 0.7266259789466858,
    "learning_rate": 5.8983050847457634e-05,
    "epoch": 0.9416941694169417,
    "step": 214
  },
  {
    "loss": 2.948,
    "grad_norm": 0.7415464520454407,
    "learning_rate": 5.830508474576272e-05,
    "epoch": 0.9460946094609461,
    "step": 215
  },
  {
    "loss": 2.8343,
    "grad_norm": 0.7127269506454468,
    "learning_rate": 5.76271186440678e-05,
    "epoch": 0.9504950495049505,
    "step": 216
  },
  {
    "loss": 2.7376,
    "grad_norm": 0.6893705725669861,
    "learning_rate": 5.6949152542372884e-05,
    "epoch": 0.9548954895489549,
    "step": 217
  },
  {
    "loss": 2.7527,
    "grad_norm": 0.7526700496673584,
    "learning_rate": 5.627118644067797e-05,
    "epoch": 0.9592959295929593,
    "step": 218
  },
  {
    "loss": 2.9258,
    "grad_norm": 0.6687633395195007,
    "learning_rate": 5.5593220338983056e-05,
    "epoch": 0.9636963696369637,
    "step": 219
  },
  {
    "loss": 3.0787,
    "grad_norm": 0.7163532972335815,
    "learning_rate": 5.4915254237288135e-05,
    "epoch": 0.9680968096809681,
    "step": 220
  },
  {
    "loss": 3.0683,
    "grad_norm": 0.7112114429473877,
    "learning_rate": 5.423728813559322e-05,
    "epoch": 0.9724972497249725,
    "step": 221
  },
  {
    "loss": 2.699,
    "grad_norm": 0.8850728273391724,
    "learning_rate": 5.355932203389831e-05,
    "epoch": 0.976897689768977,
    "step": 222
  },
  {
    "loss": 2.9018,
    "grad_norm": 0.7848578691482544,
    "learning_rate": 5.288135593220339e-05,
    "epoch": 0.9812981298129813,
    "step": 223
  },
  {
    "loss": 2.8365,
    "grad_norm": 0.7344391345977783,
    "learning_rate": 5.220338983050848e-05,
    "epoch": 0.9856985698569857,
    "step": 224
  },
  {
    "loss": 2.9117,
    "grad_norm": 0.7995946407318115,
    "learning_rate": 5.152542372881356e-05,
    "epoch": 0.9900990099009901,
    "step": 225
  },
  {
    "loss": 2.8817,
    "grad_norm": 0.8058663606643677,
    "learning_rate": 5.0847457627118643e-05,
    "epoch": 0.9944994499449945,
    "step": 226
  },
  {
    "loss": 3.0481,
    "grad_norm": 0.7356961369514465,
    "learning_rate": 5.016949152542373e-05,
    "epoch": 0.9988998899889989,
    "step": 227
  },
  {
    "loss": 3.1122,
    "grad_norm": 1.730942726135254,
    "learning_rate": 4.9491525423728815e-05,
    "epoch": 1.0,
    "step": 228
  },
  {
    "loss": 2.8877,
    "grad_norm": 0.8082904815673828,
    "learning_rate": 4.88135593220339e-05,
    "epoch": 1.0044004400440043,
    "step": 229
  },
  {
    "loss": 2.6498,
    "grad_norm": 0.8012709617614746,
    "learning_rate": 4.813559322033899e-05,
    "epoch": 1.0088008800880088,
    "step": 230
  },
  {
    "loss": 2.7348,
    "grad_norm": 0.8285991549491882,
    "learning_rate": 4.745762711864407e-05,
    "epoch": 1.0132013201320131,
    "step": 231
  },
  {
    "loss": 2.8706,
    "grad_norm": 0.9282028079032898,
    "learning_rate": 4.677966101694916e-05,
    "epoch": 1.0176017601760177,
    "step": 232
  },
  {
    "loss": 3.0262,
    "grad_norm": 0.8191248178482056,
    "learning_rate": 4.610169491525424e-05,
    "epoch": 1.022002200220022,
    "step": 233
  },
  {
    "loss": 2.9057,
    "grad_norm": 0.7432717680931091,
    "learning_rate": 4.542372881355932e-05,
    "epoch": 1.0264026402640265,
    "step": 234
  },
  {
    "loss": 2.9107,
    "grad_norm": 0.7145431637763977,
    "learning_rate": 4.474576271186441e-05,
    "epoch": 1.0308030803080308,
    "step": 235
  },
  {
    "loss": 2.6887,
    "grad_norm": 0.9042146801948547,
    "learning_rate": 4.4067796610169495e-05,
    "epoch": 1.0352035203520351,
    "step": 236
  },
  {
    "loss": 2.7061,
    "grad_norm": 0.7707528471946716,
    "learning_rate": 4.3389830508474574e-05,
    "epoch": 1.0396039603960396,
    "step": 237
  },
  {
    "loss": 2.9569,
    "grad_norm": 0.782444953918457,
    "learning_rate": 4.271186440677966e-05,
    "epoch": 1.044004400440044,
    "step": 238
  },
  {
    "loss": 2.5481,
    "grad_norm": 0.6637041568756104,
    "learning_rate": 4.2033898305084746e-05,
    "epoch": 1.0484048404840485,
    "step": 239
  },
  {
    "loss": 2.906,
    "grad_norm": 0.6901230812072754,
    "learning_rate": 4.135593220338983e-05,
    "epoch": 1.0528052805280528,
    "step": 240
  },
  {
    "loss": 2.8633,
    "grad_norm": 0.7510170340538025,
    "learning_rate": 4.067796610169492e-05,
    "epoch": 1.0572057205720573,
    "step": 241
  },
  {
    "loss": 2.7923,
    "grad_norm": 0.698947548866272,
    "learning_rate": 4e-05,
    "epoch": 1.0616061606160616,
    "step": 242
  },
  {
    "loss": 2.8105,
    "grad_norm": 0.7678452730178833,
    "learning_rate": 3.932203389830509e-05,
    "epoch": 1.066006600660066,
    "step": 243
  },
  {
    "loss": 3.0238,
    "grad_norm": 0.8770193457603455,
    "learning_rate": 3.8644067796610175e-05,
    "epoch": 1.0704070407040704,
    "step": 244
  },
  {
    "loss": 2.7161,
    "grad_norm": 0.6341068744659424,
    "learning_rate": 3.7966101694915254e-05,
    "epoch": 1.0748074807480748,
    "step": 245
  },
  {
    "loss": 2.7809,
    "grad_norm": 0.7229275107383728,
    "learning_rate": 3.728813559322034e-05,
    "epoch": 1.0792079207920793,
    "step": 246
  },
  {
    "loss": 2.9614,
    "grad_norm": 0.8525018095970154,
    "learning_rate": 3.6610169491525426e-05,
    "epoch": 1.0836083608360836,
    "step": 247
  },
  {
    "loss": 3.0395,
    "grad_norm": 0.8062288761138916,
    "learning_rate": 3.593220338983051e-05,
    "epoch": 1.0880088008800881,
    "step": 248
  },
  {
    "loss": 2.9299,
    "grad_norm": 0.8091844320297241,
    "learning_rate": 3.52542372881356e-05,
    "epoch": 1.0924092409240924,
    "step": 249
  },
  {
    "loss": 3.0747,
    "grad_norm": 0.6885312795639038,
    "learning_rate": 3.4576271186440676e-05,
    "epoch": 1.0968096809680967,
    "step": 250
  },
  {
    "loss": 2.7809,
    "grad_norm": 0.6973123550415039,
    "learning_rate": 3.389830508474576e-05,
    "epoch": 1.1012101210121013,
    "step": 251
  },
  {
    "loss": 2.8593,
    "grad_norm": 0.7132459282875061,
    "learning_rate": 3.322033898305085e-05,
    "epoch": 1.1056105610561056,
    "step": 252
  },
  {
    "loss": 2.6083,
    "grad_norm": 0.9022848606109619,
    "learning_rate": 3.2542372881355934e-05,
    "epoch": 1.11001100110011,
    "step": 253
  },
  {
    "loss": 2.8019,
    "grad_norm": 0.8500057458877563,
    "learning_rate": 3.186440677966101e-05,
    "epoch": 1.1144114411441144,
    "step": 254
  },
  {
    "loss": 2.8154,
    "grad_norm": 0.7995559573173523,
    "learning_rate": 3.1186440677966106e-05,
    "epoch": 1.118811881188119,
    "step": 255
  },
  {
    "loss": 2.9532,
    "grad_norm": 0.7603924870491028,
    "learning_rate": 3.050847457627119e-05,
    "epoch": 1.1232123212321232,
    "step": 256
  },
  {
    "loss": 2.7315,
    "grad_norm": 0.7646098732948303,
    "learning_rate": 2.9830508474576274e-05,
    "epoch": 1.1276127612761275,
    "step": 257
  },
  {
    "loss": 2.95,
    "grad_norm": 0.8374302983283997,
    "learning_rate": 2.915254237288136e-05,
    "epoch": 1.132013201320132,
    "step": 258
  },
  {
    "loss": 2.8817,
    "grad_norm": 0.7905747890472412,
    "learning_rate": 2.8474576271186442e-05,
    "epoch": 1.1364136413641364,
    "step": 259
  },
  {
    "loss": 2.647,
    "grad_norm": 0.7533621191978455,
    "learning_rate": 2.7796610169491528e-05,
    "epoch": 1.140814081408141,
    "step": 260
  },
  {
    "loss": 2.9582,
    "grad_norm": 0.6833483576774597,
    "learning_rate": 2.711864406779661e-05,
    "epoch": 1.1452145214521452,
    "step": 261
  },
  {
    "loss": 2.6203,
    "grad_norm": 0.7245395183563232,
    "learning_rate": 2.6440677966101696e-05,
    "epoch": 1.1496149614961495,
    "step": 262
  },
  {
    "loss": 2.7951,
    "grad_norm": 0.7859256267547607,
    "learning_rate": 2.576271186440678e-05,
    "epoch": 1.154015401540154,
    "step": 263
  },
  {
    "loss": 2.6427,
    "grad_norm": 0.7715824246406555,
    "learning_rate": 2.5084745762711865e-05,
    "epoch": 1.1584158415841583,
    "step": 264
  },
  {
    "loss": 2.8475,
    "grad_norm": 1.0083037614822388,
    "learning_rate": 2.440677966101695e-05,
    "epoch": 1.1628162816281629,
    "step": 265
  },
  {
    "loss": 3.0468,
    "grad_norm": 0.8350604176521301,
    "learning_rate": 2.3728813559322036e-05,
    "epoch": 1.1672167216721672,
    "step": 266
  },
  {
    "loss": 2.7074,
    "grad_norm": 0.7884132266044617,
    "learning_rate": 2.305084745762712e-05,
    "epoch": 1.1716171617161717,
    "step": 267
  },
  {
    "loss": 2.8296,
    "grad_norm": 0.7949880957603455,
    "learning_rate": 2.2372881355932205e-05,
    "epoch": 1.176017601760176,
    "step": 268
  },
  {
    "loss": 2.8361,
    "grad_norm": 0.8638668060302734,
    "learning_rate": 2.1694915254237287e-05,
    "epoch": 1.1804180418041805,
    "step": 269
  },
  {
    "loss": 2.9566,
    "grad_norm": 0.8941164612770081,
    "learning_rate": 2.1016949152542373e-05,
    "epoch": 1.1848184818481848,
    "step": 270
  },
  {
    "loss": 3.0477,
    "grad_norm": 0.7883132100105286,
    "learning_rate": 2.033898305084746e-05,
    "epoch": 1.1892189218921891,
    "step": 271
  },
  {
    "loss": 2.9396,
    "grad_norm": 0.909564197063446,
    "learning_rate": 1.9661016949152545e-05,
    "epoch": 1.1936193619361937,
    "step": 272
  },
  {
    "loss": 3.0829,
    "grad_norm": 0.7689650058746338,
    "learning_rate": 1.8983050847457627e-05,
    "epoch": 1.198019801980198,
    "step": 273
  },
  {
    "loss": 2.9316,
    "grad_norm": 0.6569347381591797,
    "learning_rate": 1.8305084745762713e-05,
    "epoch": 1.2024202420242025,
    "step": 274
  },
  {
    "loss": 2.8695,
    "grad_norm": 0.7012641429901123,
    "learning_rate": 1.76271186440678e-05,
    "epoch": 1.2068206820682068,
    "step": 275
  },
  {
    "loss": 2.8937,
    "grad_norm": 0.7256876826286316,
    "learning_rate": 1.694915254237288e-05,
    "epoch": 1.2112211221122111,
    "step": 276
  },
  {
    "loss": 3.0664,
    "grad_norm": 0.8624176383018494,
    "learning_rate": 1.6271186440677967e-05,
    "epoch": 1.2156215621562156,
    "step": 277
  },
  {
    "loss": 2.8648,
    "grad_norm": 0.8784019947052002,
    "learning_rate": 1.5593220338983053e-05,
    "epoch": 1.22002200220022,
    "step": 278
  },
  {
    "loss": 2.8395,
    "grad_norm": 0.9057832956314087,
    "learning_rate": 1.4915254237288137e-05,
    "epoch": 1.2244224422442245,
    "step": 279
  },
  {
    "loss": 2.9295,
    "grad_norm": 0.8114292621612549,
    "learning_rate": 1.4237288135593221e-05,
    "epoch": 1.2288228822882288,
    "step": 280
  },
  {
    "loss": 2.7919,
    "grad_norm": 0.8329731822013855,
    "learning_rate": 1.3559322033898305e-05,
    "epoch": 1.2332233223322333,
    "step": 281
  },
  {
    "loss": 2.9775,
    "grad_norm": 0.6972786784172058,
    "learning_rate": 1.288135593220339e-05,
    "epoch": 1.2376237623762376,
    "step": 282
  },
  {
    "loss": 2.8735,
    "grad_norm": 0.6665365099906921,
    "learning_rate": 1.2203389830508475e-05,
    "epoch": 1.2420242024202421,
    "step": 283
  },
  {
    "loss": 2.8968,
    "grad_norm": 0.7327558398246765,
    "learning_rate": 1.152542372881356e-05,
    "epoch": 1.2464246424642464,
    "step": 284
  },
  {
    "loss": 2.9608,
    "grad_norm": 0.8009313941001892,
    "learning_rate": 1.0847457627118644e-05,
    "epoch": 1.2508250825082508,
    "step": 285
  },
  {
    "loss": 2.7769,
    "grad_norm": 0.8548413515090942,
    "learning_rate": 1.016949152542373e-05,
    "epoch": 1.2552255225522553,
    "step": 286
  },
  {
    "loss": 2.8499,
    "grad_norm": 0.7384839653968811,
    "learning_rate": 9.491525423728814e-06,
    "epoch": 1.2596259625962596,
    "step": 287
  },
  {
    "loss": 2.6931,
    "grad_norm": 0.8351026177406311,
    "learning_rate": 8.8135593220339e-06,
    "epoch": 1.2640264026402641,
    "step": 288
  },
  {
    "loss": 2.7477,
    "grad_norm": 0.7270179986953735,
    "learning_rate": 8.135593220338983e-06,
    "epoch": 1.2684268426842684,
    "step": 289
  },
  {
    "loss": 2.5962,
    "grad_norm": 0.9050284624099731,
    "learning_rate": 7.4576271186440685e-06,
    "epoch": 1.2728272827282727,
    "step": 290
  },
  {
    "loss": 2.7717,
    "grad_norm": 0.8230634927749634,
    "learning_rate": 6.779661016949153e-06,
    "epoch": 1.2772277227722773,
    "step": 291
  },
  {
    "loss": 2.9172,
    "grad_norm": 0.6523515582084656,
    "learning_rate": 6.101694915254238e-06,
    "epoch": 1.2816281628162816,
    "step": 292
  },
  {
    "loss": 2.7425,
    "grad_norm": 0.7251272797584534,
    "learning_rate": 5.423728813559322e-06,
    "epoch": 1.286028602860286,
    "step": 293
  },
  {
    "loss": 2.5301,
    "grad_norm": 0.8637908101081848,
    "learning_rate": 4.745762711864407e-06,
    "epoch": 1.2904290429042904,
    "step": 294
  },
  {
    "loss": 2.7759,
    "grad_norm": 0.760532796382904,
    "learning_rate": 4.067796610169492e-06,
    "epoch": 1.2948294829482947,
    "step": 295
  },
  {
    "loss": 2.6747,
    "grad_norm": 0.7362741231918335,
    "learning_rate": 3.3898305084745763e-06,
    "epoch": 1.2992299229922992,
    "step": 296
  },
  {
    "loss": 2.9637,
    "grad_norm": 1.0277369022369385,
    "learning_rate": 2.711864406779661e-06,
    "epoch": 1.3036303630363038,
    "step": 297
  },
  {
    "loss": 3.0595,
    "grad_norm": 0.7058937549591064,
    "learning_rate": 2.033898305084746e-06,
    "epoch": 1.308030803080308,
    "step": 298
  },
  {
    "loss": 2.7173,
    "grad_norm": 1.0858601331710815,
    "learning_rate": 1.3559322033898304e-06,
    "epoch": 1.3124312431243124,
    "step": 299
  },
  {
    "loss": 2.9097,
    "grad_norm": 0.7245047092437744,
    "learning_rate": 6.779661016949152e-07,
    "epoch": 1.316831683168317,
    "step": 300
  },
  {
    "train_runtime": 311.4465,
    "train_samples_per_second": 7.706,
    "train_steps_per_second": 0.963,
    "total_flos": 7913624553332736.0,
    "train_loss": 3.0042300176620484,
    "epoch": 1.316831683168317,
    "step": 300
  }
]
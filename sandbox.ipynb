{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 0: Le Sandbox Poétique\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chargement du modèle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Chemin vers le modèle téléchargé\n",
        "MODEL_PATH = \"./models/Llama-3.2-1B-Instruct.Q4_K_M.gguf\"\n",
        "\n",
        "# Vérifier que le modèle existe\n",
        "if not Path(MODEL_PATH).exists():\n",
        "    print(f\"erreur: Le modèle n'a pas été trouvé à {MODEL_PATH}\")\n",
        "    print(\"run python download_model.py\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# n_ctx=2048 signifie que le modèle peut se souvenir d'environ 1500 mots de conversation\n",
        "llm = Llama(\n",
        "    model_path=MODEL_PATH,\n",
        "    n_ctx=2048,\n",
        "    verbose=False  # True : voir les statistiques de vitesse\n",
        ")\n",
        "\n",
        "print(\"done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Génération d'un sonnet sur 'le silence de l'hiver' ---\n",
            "\n",
            "Amidst the silence of the winter's night,\n",
            "When snowflakes dance, and frosty mists unfold,\n",
            "The hibernating world, in icy light,\n",
            "Lies slumbering, lost, in frozen fold.\n",
            "\n",
            "The wind, a mournful sigh, whispers low,\n",
            "Through trees, that creak and groan, in frozen sleep,\n",
            "Their branches swayed, like skeletal foes,\n",
            "As winter's grip, on earth, does keep.\n",
            "\n",
            "The earth, in stillness, lies, frozen deep,\n",
            "And in that silence, fears arise,\n",
            "The trees, like sentinels, stand watch and keep,\n",
            "Their vigil, through the icy winter's guise.\n",
            "\n",
            "And I, in awe, of this winter's peace,\n",
            "Am bound to silence, and the icy breeze.\n"
          ]
        }
      ],
      "source": [
        "TOPIC = \"le silence de l'hiver\"\n",
        "FORM = \"sonnet\"  # \"sonnet\", \"limerick\", \"poème en vers libres\"\n",
        "\n",
        "prompt_structure = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a skilled poet.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Write a {FORM} about {TOPIC}.\"}\n",
        "]\n",
        "\n",
        "print(f\"\\n--- Génération d'un {FORM} sur '{TOPIC}' ---\\n\")\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages=prompt_structure,\n",
        "    max_tokens=1000,   \n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "poem = output[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "print(poem)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sauvegarder le poème \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "597"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "output_file = Path(\"poem.txt\")\n",
        "output_file.write_text(poem, encoding=\"utf-8\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
